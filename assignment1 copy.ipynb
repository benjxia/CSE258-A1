{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from rankfm.rankfm import RankFM\n",
    "from fastFM import als, sgd\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [],
   "source": [
    "user_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5, handle_unknown='use_encoded_value', unknown_value=6710)\n",
    "item_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "\n",
    "itemset = set() # Set of all unique users\n",
    "userset = set() # Set of all unique items\n",
    "U = defaultdict(set) # Users that played item i\n",
    "I = defaultdict(set) # Items played by uer u\n",
    "time_played = defaultdict(dict)\n",
    "\n",
    "ft = ['early_access', 'compensation'] # features unavailable/cannot be approximated in inference\n",
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data():\n",
    "    global itemset, userset, U, I\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "        time_played[entry['userID']][entry['gameID']] = entry['hours_transformed']\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "    del data\n",
    "\n",
    "    itemset = set(df['gameID'].unique())\n",
    "    userset = set(df['userID'].unique())\n",
    "\n",
    "    U = dict(df.groupby('gameID')['userID'].unique())\n",
    "    I = dict(df.groupby('userID')['gameID'].unique())\n",
    "    U = { g : set(U[g]) for g in U }\n",
    "    I = { u : set(I[u]) for u in I }\n",
    "\n",
    "    df['userIDX'] = user_oe.fit_transform(df[['userID']])\n",
    "    df['itemIDX'] = item_oe.fit_transform(df[['gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "    # Get features that won't be available\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = process_data()\n",
    "user_mean_ft = df.groupby('userIDX')[ft].mean()\n",
    "item_mean_ft = df.groupby('itemIDX')[ft].mean()\n",
    "df.drop(labels=ft + ['found_funny'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustoi = dict(df.groupby('userID')['userIDX'].unique().apply(lambda x: x[0]))\n",
    "istoi = dict(df.groupby('itemID')['itemIDX'].unique().apply(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        dftext = df.groupby('itemIDX')['text'].apply(' '.join).reset_index()\n",
    "        counter = feature_extraction.text.CountVectorizer(min_df=0.05, max_df=0.5, stop_words='english', max_features=2000, ngram_range=(1, 2))\n",
    "        wordcount = counter.fit_transform(dftext['text'])\n",
    "        LDA = LatentDirichletAllocation(n_components=20, random_state=RANDOM_SEED)\n",
    "        text_embed = LDA.fit_transform(wordcount)\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "item_features = get_text_embedding()\n",
    "# text_embed = text_embed / np.linalg.norm(text_embed, axis=1)[...,None]\n",
    "\n",
    "df.drop('text', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = np.concatenate((np.arange(0, len(item_features))[:,  None], item_features, item_mean_ft.to_numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:150000]\n",
    "df_valid = df.iloc[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new validation set w/ negative pairs\n",
    "def gen_validation(df_valid):\n",
    "    neg_pairs = []\n",
    "    for review in df_valid.iterrows():\n",
    "        review = review[1]\n",
    "        sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "        neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "    pos_pairs = df_valid[['userIDX', 'itemIDX']].to_numpy()\n",
    "    neg_pairs = np.array(neg_pairs)\n",
    "    return pos_pairs, neg_pairs\n",
    "\n",
    "pos_pairs, neg_pairs = gen_validation(df_valid)\n",
    "\n",
    "def played_validate(model):\n",
    "    pos_scores = model.predict(pos_pairs)\n",
    "    neg_scores = model.predict(neg_pairs)\n",
    "    acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "    print(f'Validation %: {acc * 100}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation %: 69.91679667186688\n",
      "Validation %: 70.3168126725069\n",
      "Validation %: 71.06284251370055\n",
      "Validation %: 71.50286011440457\n",
      "Validation %: 71.59686387455497\n",
      "Validation %: 71.26285051402056\n",
      "Validation %: 71.41685667426697\n",
      "Validation %: 71.48485939437577\n",
      "Validation %: 71.45885835433418\n",
      "Validation %: 71.51486059442378\n",
      "Validation %: 71.52886115444618\n",
      "Validation %: 71.7528701148046\n",
      "Validation %: 72.06288251530061\n",
      "Validation %: 71.7948717948718\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1 copy.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     played_model\u001b[39m.\u001b[39mfit_partial(df[[\u001b[39m'\u001b[39m\u001b[39muserIDX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mitemIDX\u001b[39m\u001b[39m'\u001b[39m]], item_features\u001b[39m=\u001b[39mitem_features, epochs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:            \u001b[39m# Train only on training set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     played_model\u001b[39m.\u001b[39;49mfit_partial(df_train[[\u001b[39m'\u001b[39;49m\u001b[39muserIDX\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mitemIDX\u001b[39;49m\u001b[39m'\u001b[39;49m]], item_features\u001b[39m=\u001b[39;49mitem_features, epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m acc \u001b[39m=\u001b[39m played_validate(played_model)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m acc \u001b[39m>\u001b[39m best_acc:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/rankfm/rankfm.py:303\u001b[0m, in \u001b[0;36mRankFM.fit_partial\u001b[0;34m(self, interactions, user_features, item_features, sample_weight, epochs, verbose)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m[loss] function not recognized\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[39m# NOTE: the cython internal fit method updates the model weights in place via memoryviews\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m _fit(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minteractions,\n\u001b[1;32m    305\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_weight,\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_items,\n\u001b[1;32m    307\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_uf,\n\u001b[1;32m    308\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_if,\n\u001b[1;32m    309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_i,\n\u001b[1;32m    310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_if,\n\u001b[1;32m    311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_u,\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_i,\n\u001b[1;32m    313\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_uf,\n\u001b[1;32m    314\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_if,\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta,\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_schedule,\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_exponent,\n\u001b[1;32m    320\u001b[0m     max_samples,\n\u001b[1;32m    321\u001b[0m     epochs,\n\u001b[1;32m    322\u001b[0m     verbose\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32mrankfm/_rankfm.pyx:331\u001b[0m, in \u001b[0;36mrankfm._rankfm._fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrankfm/_rankfm.pyx:98\u001b[0m, in \u001b[0;36mrankfm._rankfm.assert_finite\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "save = False\n",
    "played_model = RankFM(factors=5, # Hyperparameters tuned from cross-validation\n",
    "        loss='warp',\n",
    "        max_samples=300,\n",
    "        learning_exponent=0.25,\n",
    "        learning_schedule='invscaling')\n",
    "if train == True:\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    for i in range(50):\n",
    "        # switch fit_partial's dataframe to df_train for testing, \"df\" for actual predictions\n",
    "        if test == True: # Train on entire dataset\n",
    "            played_model.fit_partial(df[['userIDX', 'itemIDX']], item_features=item_features, epochs=4, verbose=False)\n",
    "        else:            # Train only on training set\n",
    "            played_model.fit_partial(df_train[['userIDX', 'itemIDX']], item_features=item_features, epochs=4, verbose=False)\n",
    "        acc = played_validate(played_model)\n",
    "        if acc > best_acc:\n",
    "            best_model = copy.deepcopy(played_model)\n",
    "            best_acc = acc\n",
    "    if save == True:\n",
    "        model_file = open('rankfm.obj', 'wb')\n",
    "        pickle.dump(best_model, model_file)\n",
    "        model_file.close()\n",
    "else:\n",
    "    model_file = open('rankfm.obj', 'rb')\n",
    "    best_model = pickle.load(model_file)\n",
    "    played_model = best_model\n",
    "    model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred = test.copy()\n",
    "# test['itemID'] = test['gameID']\n",
    "# # Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "# test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "# test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "# test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "# test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "# scores = best_model.predict(test[['userIDX', 'itemIDX']])\n",
    "# testpred = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred['prediction'] = (scores >= np.median(scores)).astype(np.int32)\n",
    "# testpred.to_csv('./predictions_Played.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0\n",
    "# beta_u = np.zeros(len(df['userIDX'].unique()))\n",
    "# beta_i = np.zeros(len(df['itemIDX'].unique()))\n",
    "user_oe2 = preprocessing.OrdinalEncoder(dtype=np.int32)\n",
    "item_oe2 = preprocessing.OrdinalEncoder(dtype=np.int32)\n",
    "df['userIDX2'] = user_oe2.fit_transform(df[['userID']])\n",
    "df['itemIDX2'] = item_oe2.fit_transform(df[['itemID']])\n",
    "u_cnt = dict(df.iloc[:150000].groupby('userIDX2')['itemIDX2'].count())\n",
    "i_cnt = dict(df.iloc[:150000].groupby('itemIDX2')['userIDX2'].count())\n",
    "beta_u = np.zeros(len(I))\n",
    "beta_i = np.zeros(len(U))\n",
    "alpha = df['hours_transformed'].mean()\n",
    "def closed_form(lamb, trainset):\n",
    "    global alpha\n",
    "    global beta_u\n",
    "    global beta_i\n",
    "\n",
    "    new_beta_u = np.zeros_like(beta_u)\n",
    "    new_beta_i = np.zeros_like(beta_i)\n",
    "    new_alpha = 0\n",
    "\n",
    "    for i in range(len(trainset)):\n",
    "        new_alpha += (trainset[i, -1] - beta_u[trainset[i, 0]] - beta_i[trainset[i, 1]]) / len(trainset)\n",
    "    alpha = new_alpha\n",
    "    for i in range(len(trainset)):\n",
    "        new_beta_u[trainset[i, 0]] += (trainset[i, -1] - alpha - beta_i[trainset[i, 1]]) / (lamb + u_cnt[trainset[i, 0]])\n",
    "    beta_u = new_beta_u\n",
    "    for i in range(len(trainset)):\n",
    "        new_beta_i[trainset[i, 1]] += (trainset[i, -1] - alpha - beta_u[trainset[i, 0]]) / (lamb + i_cnt[trainset[i, 1]])\n",
    "    beta_i = new_beta_i\n",
    "\n",
    "def validate(validset, alpha, beta_u, beta_i):\n",
    "    label = validset['hours_transformed'].to_numpy()\n",
    "    validset = validset[['userIDX2', 'itemIDX2']].to_numpy()\n",
    "    preds = alpha + beta_u[validset[:, 0].astype(np.int32)] + beta_i[validset[:, 1].astype(np.int32)]\n",
    "    return np.mean((preds - label)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    closed_form(5, df.iloc[:150000][['userIDX2', 'itemIDX2', 'hours_transformed']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.7153235425111957,\n",
       " array([ 0.18589349, -0.56707517,  1.09466634, ...,  0.46196124,\n",
       "         0.80780987,  0.97348675]),\n",
       " array([-0.43888858, -0.5976366 , -1.22902702, ..., -0.04556356,\n",
       "        -1.46630672,  0.56756049]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_cnt = dict(df.iloc[:150000].groupby('userIDX2')['itemIDX2'].count())\n",
    "i_cnt = dict(df.iloc[:150000].groupby('itemIDX2')['userIDX2'].count())\n",
    "closed_form(5, df.iloc[:150000], 0, np.zeros(len(df['userIDX2'].unique())),  np.zeros(len(df['itemIDX2'].unique())), u_cnt, i_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "closed_form() takes 2 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m i_cnt \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(df\u001b[39m.\u001b[39miloc[train]\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mitemIDX2\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39muserIDX2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique())\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     alpha, beta_u, beta_i \u001b[39m=\u001b[39m closed_form(\u001b[39m5\u001b[39;49m, df\u001b[39m.\u001b[39;49miloc[train], alpha, beta_u, beta_i, u_cnt, i_cnt)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlamb = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m5\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mfold = \u001b[39m\u001b[39m{\u001b[39;00mit\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mMSE = \u001b[39m\u001b[39m{\u001b[39;00mvalidate(df\u001b[39m.\u001b[39miloc[test],\u001b[39m \u001b[39malpha,\u001b[39m \u001b[39mbeta_u,\u001b[39m \u001b[39mbeta_i)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: closed_form() takes 2 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "for it, (train, test) in enumerate(splitter.split(df)):\n",
    "    alpha = 0\n",
    "    beta_u = np.zeros(len(df['userIDX2'].unique()))\n",
    "    beta_i = np.zeros(len(df['itemIDX2'].unique()))\n",
    "    u_cnt = dict(df.iloc[train].groupby('userIDX2')['itemIDX2'].nunique())\n",
    "    i_cnt = dict(df.iloc[train].groupby('itemIDX2')['userIDX2'].nunique())\n",
    "    for i in range(500):\n",
    "        alpha, beta_u, beta_i = closed_form(5, df.iloc[train], alpha, beta_u, beta_i, u_cnt, i_cnt)\n",
    "        if i % 100 == 0:\n",
    "            print(f'lamb = {5}\\tfold = {it}\\tMSE = {validate(df.iloc[test], alpha, beta_u, beta_i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "closed_form() missing 2 required positional arguments: 'u_cnt' and 'i_cnt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb Cell 21\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m beta_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mitemIDX\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     alpha, beta_u, beta_i \u001b[39m=\u001b[39m closed_form(i, df\u001b[39m.\u001b[39;49miloc[train], alpha, beta_u, beta_i)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlamb = \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mit = \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mMSE = \u001b[39m\u001b[39m{\u001b[39;00mvalidate(df\u001b[39m.\u001b[39miloc[test],\u001b[39m \u001b[39malpha,\u001b[39m \u001b[39mbeta_u,\u001b[39m \u001b[39mbeta_i)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/benjx/cs_wsl/school/y3/cse258/CSE258-A1/assignment1.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: closed_form() missing 2 required positional arguments: 'u_cnt' and 'i_cnt'"
     ]
    }
   ],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "for i in range(10): # Lambda\n",
    "    for train, test in splitter.split(df):\n",
    "        alpha = 0\n",
    "        beta_u = np.zeros(len(df['userIDX'].unique()))\n",
    "        beta_i = np.zeros(len(df['itemIDX'].unique()))\n",
    "        for j in range(10):\n",
    "            alpha, beta_u, beta_i = closed_form(i, df.iloc[train], alpha, beta_u, beta_i)\n",
    "            print(f'lamb = {i}\\tit = {j}\\tMSE = {validate(df.iloc[test], alpha, beta_u, beta_i)}')\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred = test.copy()\n",
    "test['itemID'] = test['gameID']\n",
    "# Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "\n",
    "time_test = convert_df(test)\n",
    "preds = time_model.predict(time_test)\n",
    "\n",
    "testpred = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred['prediction'] = preds\n",
    "testpred.to_csv('./predictions_Hours.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

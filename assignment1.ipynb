{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efffccb8e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>early_access</th>\n",
       "      <th>hours_transformed</th>\n",
       "      <th>found_funny</th>\n",
       "      <th>text</th>\n",
       "      <th>itemID</th>\n",
       "      <th>compensation</th>\n",
       "      <th>userIDX</th>\n",
       "      <th>itemIDX</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>day_of_month_cos</th>\n",
       "      <th>day_of_month_sin</th>\n",
       "      <th>day_of_wk_cos</th>\n",
       "      <th>day_of_wk_sin</th>\n",
       "      <th>day_of_yr_cos</th>\n",
       "      <th>day_of_yr_sin</th>\n",
       "      <th>wk_of_yr_cos</th>\n",
       "      <th>wk_of_yr_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u70666506</td>\n",
       "      <td>0</td>\n",
       "      <td>6.011227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>If you want to sit in queue for 10-20min and h...</td>\n",
       "      <td>g49368897</td>\n",
       "      <td>0</td>\n",
       "      <td>4740</td>\n",
       "      <td>1209</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654861</td>\n",
       "      <td>0.75575</td>\n",
       "      <td>-0.669131</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.732494</td>\n",
       "      <td>0.680773</td>\n",
       "      <td>-0.663123</td>\n",
       "      <td>0.748511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u18612571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0</td>\n",
       "      <td>I was really not a fan of the gameplay. Games ...</td>\n",
       "      <td>g73495588</td>\n",
       "      <td>0</td>\n",
       "      <td>1240</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.901502</td>\n",
       "      <td>0.432776</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>0.354605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u34283088</td>\n",
       "      <td>0</td>\n",
       "      <td>3.689299</td>\n",
       "      <td>0</td>\n",
       "      <td>Vaas Montenegro is the reason why you should g...</td>\n",
       "      <td>g68047320</td>\n",
       "      <td>0</td>\n",
       "      <td>2314</td>\n",
       "      <td>1652</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415415</td>\n",
       "      <td>0.909632</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452072</td>\n",
       "      <td>0.891981</td>\n",
       "      <td>0.464723</td>\n",
       "      <td>0.885456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u16220374</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263034</td>\n",
       "      <td>0</td>\n",
       "      <td>8/10 Wonderful game, simple controls and platf...</td>\n",
       "      <td>g51234623</td>\n",
       "      <td>0</td>\n",
       "      <td>1067</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.959493</td>\n",
       "      <td>0.281733</td>\n",
       "      <td>-0.809017</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.944188</td>\n",
       "      <td>0.329408</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>0.354605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u01499286</td>\n",
       "      <td>0</td>\n",
       "      <td>1.432959</td>\n",
       "      <td>0</td>\n",
       "      <td>Never knew a guns had THAT many parts!</td>\n",
       "      <td>g25723374</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.978148</td>\n",
       "      <td>-0.207912</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.962309</td>\n",
       "      <td>0.271958</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>0.239316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID  early_access hours_transformed found_funny  \\\n",
       "0  u70666506             0          6.011227         1.0   \n",
       "1  u18612571             0          0.263034           0   \n",
       "2  u34283088             0          3.689299           0   \n",
       "3  u16220374             0          1.263034           0   \n",
       "4  u01499286             0          1.432959           0   \n",
       "\n",
       "                                                text     itemID  compensation  \\\n",
       "0  If you want to sit in queue for 10-20min and h...  g49368897             0   \n",
       "1  I was really not a fan of the gameplay. Games ...  g73495588             0   \n",
       "2  Vaas Montenegro is the reason why you should g...  g68047320             0   \n",
       "3  8/10 Wonderful game, simple controls and platf...  g51234623             0   \n",
       "4             Never knew a guns had THAT many parts!  g25723374             0   \n",
       "\n",
       "  userIDX itemIDX     month  ... month_cos month_sin day_of_month_cos  \\\n",
       "0    4740    1209  0.363636  ... -0.654861   0.75575        -0.669131   \n",
       "1    1240    1800       0.0  ...       1.0       0.0         0.669131   \n",
       "2    2314    1652  0.181818  ...  0.415415  0.909632              0.5   \n",
       "3    1067    1244  0.454545  ... -0.959493  0.281733        -0.809017   \n",
       "4      92     609       0.0  ...       1.0       0.0        -0.978148   \n",
       "\n",
       "  day_of_month_sin day_of_wk_cos day_of_wk_sin day_of_yr_cos day_of_yr_sin  \\\n",
       "0        -0.743145           0.5     -0.866025     -0.732494      0.680773   \n",
       "1        -0.743145          -0.5     -0.866025      0.901502      0.432776   \n",
       "2         0.866025          -1.0           0.0      0.452072      0.891981   \n",
       "3         0.587785           0.5     -0.866025     -0.944188      0.329408   \n",
       "4        -0.207912           0.5     -0.866025      0.962309      0.271958   \n",
       "\n",
       "  wk_of_yr_cos wk_of_yr_sin  \n",
       "0    -0.663123     0.748511  \n",
       "1     0.935016     0.354605  \n",
       "2     0.464723     0.885456  \n",
       "3    -0.935016     0.354605  \n",
       "4     0.970942     0.239316  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "item_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "\n",
    "itemset = set() # Set of all unique users\n",
    "userset = set() # Set of all unique items\n",
    "U = defaultdict(set)\n",
    "I = defaultdict(set)\n",
    "\n",
    "ft = ['early_access', 'hours_transformed', 'found_funny', 'compensation'] # features unavailable/cannot be approximated in inference\n",
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data():\n",
    "    global itemset, userset, U, I\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "    del data\n",
    "    itemset = set(df['gameID'].unique())\n",
    "    userset = set(df['userID'].unique())\n",
    "\n",
    "    U = dict(df.groupby('gameID')['userID'].unique())\n",
    "    I = dict(df.groupby('userID')['gameID'].unique())\n",
    "    U = { g : set(U[g]) for g in U }\n",
    "    I = { u : set(I[u]) for u in I }\n",
    "\n",
    "    df['userIDX'] = user_oe.fit_transform(df[['userID']])\n",
    "    df['itemIDX'] = item_oe.fit_transform(df[['gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "\n",
    "    # Get features for time\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].map(lambda x : x.month)\n",
    "    df['year'] = df['date'].map(lambda x : x.year)\n",
    "    df['day_of_month'] = df['date'].map(lambda x : x.day)\n",
    "    df['day_of_wk'] = df['date'].map(lambda x : x.dayofweek)\n",
    "    df['day_of_yr'] = df['date'].map(lambda x : x.dayofyear)\n",
    "    df['wk_of_yr'] = df['date'].map(lambda x : x.weekofyear)\n",
    "    mme = preprocessing.MinMaxScaler() # Normalize time to range [0, 1]\n",
    "    df[['month', 'year', 'day_of_month', 'day_of_wk', 'day_of_yr', 'wk_of_yr']] = mme.fit_transform(df[['month', 'year', 'day_of_month', 'day_of_wk', 'day_of_yr', 'wk_of_yr']])\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "    # Use Fourier features to help with representating cyclic nature of time\n",
    "    for time_unit in [ 'month', 'day_of_month', 'day_of_wk', 'day_of_yr', 'wk_of_yr']:\n",
    "        df[time_unit + '_cos'] = df[time_unit].map(lambda x: np.cos(x * 2 * np.pi))\n",
    "        df[time_unit + '_sin'] = df[time_unit].map(lambda x: np.sin(x * 2 * np.pi))\n",
    "\n",
    "\n",
    "    # Get features that won't be available\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "\n",
    "    time_label = df['hours_transformed']\n",
    "\n",
    "    return df, time_label\n",
    "\n",
    "df, time_label = process_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# NOTE: Using pretrained sentiment similarity transformer\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        transformer = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "        text_embed = np.zeros((len(df), 384))\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(df), 1)):\n",
    "                encoded_input = tokenizer(df.iloc[i:i+1]['text'].tolist(),\n",
    "                                        padding=True,\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt').to(device)\n",
    "                model_output = transformer(**encoded_input)\n",
    "                embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                embeddings: torch.Tensor = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "                text_embed[i:i+1] = embeddings.cpu().numpy()\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "text_embed = get_text_embedding()\n",
    "\n",
    "text_cols = ['te_' + str(i) for i in range(text_embed.shape[1])]\n",
    "\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "\n",
    "# Add text descriptor features to dataframe\n",
    "df = df.join(pd.DataFrame(text_embed, columns=text_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mean = df.groupby('userIDX')[ft + text_cols].mean()\n",
    "item_mean = df.groupby('itemIDX')[ft + text_cols].mean()\n",
    "df.drop(labels=ft + text_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_played_train = df.iloc[:150000]\n",
    "df_played_valid = df.iloc[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Played dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbxia/.local/lib/python3.9/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but OrdinalEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.7400e+03,  1.2090e+03,  3.6364e-01,  8.7500e-01,  6.3333e-01,\n",
       "          8.3333e-01,  3.8082e-01,  3.6538e-01, -6.5486e-01,  7.5575e-01,\n",
       "         -6.6913e-01, -7.4314e-01,  5.0000e-01, -8.6603e-01, -7.3249e-01,\n",
       "          6.8077e-01, -6.6312e-01,  7.4851e-01,  2.1429e-01,  4.9497e+00,\n",
       "          3.2643e+01,  0.0000e+00, -2.0878e-02, -7.3889e-03, -3.8625e-03,\n",
       "         -5.0535e-02, -1.1783e-02,  3.1898e-02,  6.5037e-03, -1.3966e-05,\n",
       "         -2.4322e-02,  4.0201e-02, -4.6274e-02,  2.1103e-02, -1.7140e-02,\n",
       "          1.4387e-02,  9.4856e-04, -6.8986e-03,  5.9447e-02, -2.6645e-02,\n",
       "          1.9625e-03, -1.8561e-02, -2.1035e-02, -1.9221e-02,  8.7146e-03,\n",
       "         -1.4833e-03, -7.6345e-03, -7.9339e-03, -1.9430e-02,  2.4253e-02,\n",
       "         -3.8123e-02, -4.6568e-02, -1.8670e-02,  5.1211e-02, -4.2783e-03,\n",
       "         -3.8196e-04, -4.0363e-02, -9.5039e-03,  2.4656e-02, -6.1482e-02,\n",
       "         -1.7572e-02, -1.1578e-02, -2.3224e-02, -7.8378e-03,  1.7691e-02,\n",
       "          3.5012e-02,  6.3939e-03, -1.0596e-02, -2.6779e-02, -2.5244e-02,\n",
       "          5.7578e-02,  2.6650e-02, -1.0357e-02, -2.5465e-02, -4.5545e-03,\n",
       "         -2.3285e-02,  4.3090e-02,  5.6097e-03,  3.0907e-03, -8.7786e-03,\n",
       "         -8.5674e-03, -4.9801e-02,  6.6305e-03, -2.0992e-02, -2.8705e-02,\n",
       "          4.6538e-03,  3.6479e-02, -3.0568e-02,  9.6655e-03, -1.0263e-02,\n",
       "         -1.5800e-02, -2.2179e-03, -2.2886e-02,  2.0779e-02, -4.6353e-03,\n",
       "         -2.6164e-02, -1.4870e-02,  4.2060e-02, -3.1666e-02, -7.3926e-02,\n",
       "         -3.5886e-02,  2.8824e-02,  4.2069e-02,  1.8360e-02, -5.6762e-03,\n",
       "          1.3522e-02, -3.6312e-02, -3.0230e-02,  6.3013e-03,  9.2082e-03,\n",
       "          1.3700e-02, -1.6012e-02,  1.4733e-02,  2.1459e-02,  6.1978e-02,\n",
       "          4.7832e-02, -1.3874e-02,  2.8839e-02,  1.1357e-02, -5.2838e-02,\n",
       "         -4.6085e-02,  3.5132e-02, -2.0732e-02,  4.0364e-03, -6.1810e-03,\n",
       "         -1.6299e-02,  3.3830e-02, -1.9870e-03,  3.6497e-02,  2.2285e-02,\n",
       "         -3.5198e-02, -1.5771e-02, -1.1905e-02, -2.5854e-02,  1.0018e-02,\n",
       "          1.4148e-02,  1.3409e-02,  3.2050e-02,  8.1515e-03,  2.6911e-02,\n",
       "         -1.8388e-02,  6.0402e-02,  6.1563e-02,  5.0198e-03,  7.8541e-03,\n",
       "          1.2646e-02, -2.0782e-04,  3.8431e-03,  5.1785e-02,  1.5168e-34,\n",
       "         -3.1806e-02,  1.8031e-02, -1.4244e-02,  3.2452e-02,  5.3335e-02,\n",
       "         -2.8686e-02,  1.2400e-02,  1.0659e-02, -8.0498e-02,  1.3207e-02,\n",
       "         -4.8199e-02,  1.3678e-02, -6.9429e-02,  2.5634e-02,  5.5759e-02,\n",
       "         -4.9630e-02, -1.6527e-02,  2.5205e-02,  2.1564e-03,  3.4589e-02,\n",
       "          2.9639e-03,  9.4739e-03, -1.1186e-02, -3.2119e-02, -1.1241e-02,\n",
       "          5.3897e-02, -2.3779e-02,  5.0885e-03,  6.0565e-02,  9.6461e-03,\n",
       "         -5.7980e-03, -1.2712e-02, -4.6139e-02, -1.4095e-02,  8.2618e-03,\n",
       "          7.5159e-03, -1.6599e-02, -1.1664e-02, -1.7382e-02,  3.0807e-02,\n",
       "         -3.7272e-02,  1.3737e-02, -5.7255e-02, -8.8260e-03,  3.3382e-02,\n",
       "         -1.4829e-02,  2.2916e-02, -3.7202e-02, -3.3824e-02, -4.8178e-03,\n",
       "         -4.0761e-02,  1.3029e-02, -3.5435e-02,  4.4597e-02, -2.8404e-03,\n",
       "         -2.7502e-02,  4.2838e-02, -2.8727e-02, -2.6486e-02,  3.7686e-02,\n",
       "          4.5223e-02, -3.0334e-02,  2.4167e-02, -2.9788e-02, -2.0964e-02,\n",
       "          3.0055e-02,  3.2580e-02, -2.1319e-03,  7.8603e-04,  2.6279e-03,\n",
       "         -4.2180e-03, -1.3151e-02,  4.0886e-02, -8.1530e-03,  3.0161e-02,\n",
       "          1.8256e-03, -1.0796e-02, -2.8478e-02, -2.7535e-02, -4.3817e-02,\n",
       "          2.0236e-03, -1.5002e-02, -4.1846e-02,  6.8153e-03,  1.5236e-02,\n",
       "         -7.9242e-03,  1.6552e-02, -4.7653e-02, -2.4244e-02, -1.6616e-02,\n",
       "         -5.1689e-02, -3.2699e-02,  6.7178e-02,  1.3352e-02,  1.5380e-02,\n",
       "         -8.5630e-34,  1.0105e-02, -5.2413e-02, -4.3769e-02,  2.2506e-02,\n",
       "         -2.8369e-02,  1.0062e-02, -5.1694e-02,  4.6625e-02,  1.8944e-02,\n",
       "          5.3125e-02,  5.5110e-03,  3.7211e-02,  3.0795e-02,  1.3192e-02,\n",
       "          8.1663e-03, -3.6733e-02,  4.1941e-02, -2.4025e-02,  2.4410e-02,\n",
       "          4.6545e-03,  3.2766e-02,  1.4465e-02, -3.5289e-02, -4.4349e-02,\n",
       "          2.9854e-02,  3.4643e-02, -8.4534e-03, -1.4915e-02, -5.5509e-03,\n",
       "          3.6636e-02,  2.8194e-02, -2.3140e-02, -2.8487e-03, -2.4417e-02,\n",
       "          1.0471e-02,  2.4135e-02,  5.5466e-02,  3.1472e-04, -3.7510e-02,\n",
       "         -1.6312e-03,  3.1489e-02, -4.7383e-03, -3.7572e-02,  1.0403e-02,\n",
       "         -8.1180e-03,  2.7574e-02,  4.0849e-02, -1.0259e-02, -4.0972e-03,\n",
       "          3.9324e-02, -3.9364e-03, -1.5594e-02, -1.0416e-02, -3.9075e-02,\n",
       "         -1.7335e-02, -3.5363e-02, -1.2437e-02,  1.0219e-02,  1.3514e-02,\n",
       "          9.5076e-03, -2.1119e-02,  2.4249e-02, -3.8552e-02,  3.4735e-02,\n",
       "          2.9436e-02,  2.3294e-02,  1.4683e-02,  2.1775e-03, -1.1893e-02,\n",
       "         -1.3453e-02, -7.9841e-02,  1.8987e-02, -8.0614e-02,  3.8837e-02,\n",
       "          1.4049e-03, -3.9182e-03,  3.0678e-03,  2.7553e-02,  1.1968e-02,\n",
       "          2.2470e-02, -1.7148e-02, -5.3293e-03, -1.8084e-02,  1.9227e-03,\n",
       "          2.8072e-02,  2.4643e-02,  1.3463e-02,  3.1087e-02, -7.4350e-03,\n",
       "          6.1686e-04,  2.6736e-02,  4.8789e-02,  1.5627e-02,  1.8489e-03,\n",
       "          3.4892e-02, -3.2828e-08,  2.7846e-03,  5.2239e-03,  1.2833e-02,\n",
       "         -1.5013e-02,  3.1118e-03,  7.6021e-03, -1.9648e-02,  1.6627e-02,\n",
       "         -1.9692e-02,  5.1657e-02,  4.9377e-02,  2.3822e-03, -2.4493e-02,\n",
       "          4.6655e-03,  2.2749e-02,  1.3419e-02,  5.7920e-02, -1.0677e-02,\n",
       "         -1.3138e-02,  2.3902e-02,  1.2386e-02,  3.8834e-02,  2.5430e-02,\n",
       "         -8.2210e-02, -5.9448e-02,  1.5920e-02, -1.1584e-02, -2.7883e-02,\n",
       "          5.8477e-03,  1.6924e-02,  5.5870e-02,  2.1297e-02, -1.5738e-02,\n",
       "          1.3215e-02, -1.2734e-02, -2.5090e-03,  6.8100e-03,  2.2237e-02,\n",
       "          1.0581e-02, -1.8621e-03,  9.7666e-03, -1.6306e-02, -2.5785e-02,\n",
       "         -4.0631e-03, -1.1415e-02, -1.8808e-03, -2.3845e-03, -6.5954e-02,\n",
       "         -4.1797e-03, -4.7889e-02,  1.0501e-02,  1.0621e-02, -1.4222e-02,\n",
       "          7.7441e-03,  8.4079e-02,  2.5481e-02,  2.8211e-04,  4.0834e-02,\n",
       "         -3.9023e-03,  5.0439e-02,  8.1307e-02,  4.1612e-03, -6.4782e-02,\n",
       "          2.0904e-02,  5.7895e-01,  3.4550e+00,  1.2456e+00,  2.9240e-02,\n",
       "         -2.2451e-02,  4.0948e-04,  7.4077e-03, -3.0785e-02, -1.2185e-02,\n",
       "         -6.5901e-03,  6.6579e-03,  9.4932e-03, -2.1843e-02,  4.6389e-02,\n",
       "         -2.1361e-02,  4.8183e-04, -2.1801e-02, -1.9323e-03,  3.4558e-03,\n",
       "          2.6421e-03,  5.9630e-02, -3.3384e-02, -2.1445e-03, -1.3220e-03,\n",
       "         -4.5612e-02, -2.8281e-02, -1.2575e-02, -6.5825e-03, -2.3975e-02,\n",
       "          2.8791e-03, -7.1862e-03,  2.9097e-02, -4.3688e-02, -5.2556e-02,\n",
       "         -2.3331e-03,  5.8638e-02,  1.0105e-02, -7.0463e-03, -2.6564e-02,\n",
       "          1.4232e-02,  2.1100e-02, -3.5193e-02, -3.2735e-02, -1.3863e-02,\n",
       "         -3.0411e-02, -1.1516e-03,  9.0650e-03,  4.6174e-02,  1.9211e-04,\n",
       "         -2.0479e-02, -6.5246e-03, -2.7597e-02,  4.7152e-02,  1.7061e-02,\n",
       "         -3.9557e-03, -4.6767e-02,  1.8952e-02, -2.0176e-02,  2.3975e-02,\n",
       "          3.4598e-04, -8.2974e-03, -8.1012e-04,  1.1345e-02, -2.3077e-02,\n",
       "          1.0921e-02, -1.6572e-02, -1.0619e-02,  1.6704e-02,  4.7084e-02,\n",
       "         -2.8221e-02,  1.1510e-02, -3.4420e-03, -6.5464e-03, -7.6892e-03,\n",
       "         -1.7924e-02,  2.8014e-02, -4.2896e-03, -2.0303e-03, -5.8372e-03,\n",
       "          3.7804e-02, -5.4403e-03, -5.7258e-02,  1.6063e-02,  2.8574e-02,\n",
       "          2.1704e-02,  3.5902e-03, -4.5689e-02,  1.2011e-02, -1.7400e-02,\n",
       "         -3.5093e-02,  1.4445e-03,  1.7971e-03,  3.2240e-02, -1.0975e-03,\n",
       "          2.3061e-02,  4.2196e-02,  6.3960e-02,  2.7939e-02, -2.0389e-02,\n",
       "          2.2485e-02,  1.6629e-04, -3.3024e-02, -3.6885e-02,  5.2708e-02,\n",
       "         -4.5956e-03,  1.7184e-02,  9.5850e-03, -1.5194e-02,  8.9111e-03,\n",
       "          8.1739e-03,  2.0689e-02,  3.1576e-02, -2.4310e-02, -2.8289e-02,\n",
       "         -2.9861e-02,  2.8002e-03, -2.3321e-02,  3.6466e-04, -7.4381e-03,\n",
       "          3.8657e-02, -1.6327e-02,  1.1349e-02, -2.1069e-02,  3.1806e-02,\n",
       "          5.4962e-02, -1.8337e-02,  1.0344e-02,  1.7771e-02,  1.6989e-02,\n",
       "          3.7121e-03,  2.5953e-02, -8.2482e-34, -1.0894e-02, -1.5264e-03,\n",
       "         -1.1979e-02,  1.5377e-02,  2.8469e-02, -2.4011e-02,  8.8842e-03,\n",
       "          8.5161e-04, -8.3536e-02,  1.3191e-02, -2.8606e-02,  1.7266e-02,\n",
       "         -3.7012e-02,  1.7043e-02,  7.2949e-02, -6.8763e-02, -5.5085e-03,\n",
       "          1.4886e-02,  8.1489e-03,  6.7755e-03,  9.7627e-03, -1.2485e-02,\n",
       "         -1.3363e-02, -3.5699e-02,  1.0271e-02,  5.7887e-02, -2.1662e-02,\n",
       "         -2.0105e-02,  6.0666e-02,  1.6394e-02, -4.6368e-02, -3.8944e-03,\n",
       "         -4.1595e-02, -4.2750e-04,  1.4659e-02,  7.9186e-03, -1.9890e-02,\n",
       "         -2.9127e-02, -3.9386e-02,  1.0754e-02, -4.4870e-02,  1.9314e-02,\n",
       "         -4.9259e-02,  7.5859e-03,  3.4661e-03, -1.6077e-02,  9.3357e-03,\n",
       "         -3.8786e-02, -2.5956e-02,  2.3253e-02, -2.3286e-02,  6.9667e-03,\n",
       "         -2.4353e-02,  3.4451e-02,  6.4765e-03, -6.4327e-03,  3.2411e-02,\n",
       "         -2.3468e-02, -5.9722e-03,  2.3274e-02,  2.0489e-02, -2.0097e-02,\n",
       "          1.1369e-02, -9.3903e-03, -3.8184e-03,  4.5675e-02,  2.0526e-02,\n",
       "          1.4105e-03,  7.5987e-03,  2.1848e-04,  1.1080e-02, -6.6706e-03,\n",
       "          4.0784e-02, -7.9011e-03,  2.5964e-02,  7.4350e-03, -1.2055e-04,\n",
       "         -1.7342e-02, -2.1983e-02, -1.6690e-02, -4.9022e-03, -1.1321e-02,\n",
       "         -3.3857e-02,  2.5490e-04,  1.5521e-02, -6.3221e-03,  1.0489e-02,\n",
       "         -2.5743e-02, -1.8202e-02, -3.7218e-03, -4.5910e-02, -2.8247e-02,\n",
       "          4.0407e-02,  8.0519e-03,  3.2032e-03,  8.6109e-35, -1.0939e-02,\n",
       "         -3.6359e-02, -4.1317e-02,  1.4448e-02,  3.7010e-03,  1.3830e-03,\n",
       "         -2.3942e-02,  5.0829e-02, -5.3212e-03,  4.7921e-02, -2.7441e-02,\n",
       "          4.3966e-02,  1.1470e-02,  8.8190e-03,  1.5950e-02, -3.2017e-02,\n",
       "          5.9215e-02, -8.5610e-04,  1.6981e-02, -1.5393e-02,  4.0834e-02,\n",
       "          1.8190e-02, -2.2695e-02, -2.1078e-02,  1.1396e-02,  2.7305e-02,\n",
       "         -4.3403e-03, -3.2855e-03, -1.1662e-02,  3.3363e-02,  3.5569e-02,\n",
       "          1.5661e-02, -2.3221e-05, -1.5572e-02,  1.7185e-02,  3.1459e-02,\n",
       "          3.5531e-02,  3.0075e-02, -2.7002e-02, -2.3754e-02,  8.2141e-04,\n",
       "          3.1324e-03, -5.1861e-02,  2.2484e-02, -2.5769e-02,  2.2838e-02,\n",
       "          4.3336e-02,  5.0837e-04,  4.4624e-04,  4.3960e-02, -1.2615e-02,\n",
       "         -3.2922e-02,  7.7260e-03, -5.7378e-02, -3.3020e-02, -2.7894e-02,\n",
       "         -2.5751e-02, -5.5488e-03,  1.4236e-02,  9.6222e-03,  1.3452e-02,\n",
       "          4.2382e-03, -4.5207e-02,  1.3355e-02,  2.6226e-02,  5.3340e-03,\n",
       "         -2.9381e-03,  1.2987e-02, -2.1629e-02,  8.3542e-03, -4.7590e-02,\n",
       "          3.7842e-03, -6.2881e-02,  2.2988e-02,  5.4019e-03,  2.2366e-03,\n",
       "         -7.4613e-03,  2.4706e-02,  1.9279e-02,  2.2843e-02, -1.4868e-02,\n",
       "          2.6972e-04,  5.9386e-03,  5.2151e-04,  5.6625e-03,  2.4618e-02,\n",
       "         -4.4054e-04,  2.7429e-02, -9.4481e-03, -3.6252e-03,  1.3959e-02,\n",
       "          3.3927e-02,  1.9185e-02,  2.7308e-02,  9.0035e-03, -3.3453e-08,\n",
       "          5.2124e-03,  4.8542e-03,  2.4602e-02,  8.6516e-03,  2.5901e-03,\n",
       "         -3.2908e-03, -1.2224e-02,  1.2495e-02, -2.8148e-03,  4.2884e-02,\n",
       "          2.7663e-02, -1.1221e-02, -3.0201e-02,  7.6051e-04,  2.3836e-02,\n",
       "          2.6684e-02,  1.4158e-02,  1.8114e-02, -4.4176e-02,  8.0031e-03,\n",
       "          1.3890e-02,  3.6201e-02,  3.2175e-03, -4.0960e-02, -4.3262e-02,\n",
       "          7.8975e-03, -4.6240e-03,  2.4814e-03,  1.4430e-02, -5.7236e-03,\n",
       "          2.8978e-02,  2.5424e-02, -1.1895e-02,  2.1748e-02, -2.0929e-03,\n",
       "          6.0629e-03,  5.4012e-03,  1.1541e-02, -3.1859e-03,  6.5832e-03,\n",
       "         -3.0381e-02,  2.0541e-02, -1.3826e-03, -7.7934e-03, -2.0919e-02,\n",
       "          1.6488e-02, -2.3959e-02, -5.8532e-02, -7.1677e-03, -5.0461e-02,\n",
       "          1.2799e-02,  1.0346e-02, -3.4850e-02,  1.5078e-02,  7.7849e-02,\n",
       "          3.7525e-02,  4.1457e-03,  3.4889e-02,  1.0286e-02,  2.9115e-02,\n",
       "          4.1124e-02, -2.3298e-02, -4.5149e-02,  3.5255e-02]),\n",
       " tensor([ 2.0830e+03,  1.2090e+03,  3.6364e-01,  8.7500e-01,  6.3333e-01,\n",
       "          8.3333e-01,  3.8082e-01,  3.6538e-01, -6.5486e-01,  7.5575e-01,\n",
       "         -6.6913e-01, -7.4314e-01,  5.0000e-01, -8.6603e-01, -7.3249e-01,\n",
       "          6.8077e-01, -6.6312e-01,  7.4851e-01,  2.1429e-01,  4.9497e+00,\n",
       "          3.2643e+01,  0.0000e+00, -2.0878e-02, -7.3889e-03, -3.8625e-03,\n",
       "         -5.0535e-02, -1.1783e-02,  3.1898e-02,  6.5037e-03, -1.3966e-05,\n",
       "         -2.4322e-02,  4.0201e-02, -4.6274e-02,  2.1103e-02, -1.7140e-02,\n",
       "          1.4387e-02,  9.4856e-04, -6.8986e-03,  5.9447e-02, -2.6645e-02,\n",
       "          1.9625e-03, -1.8561e-02, -2.1035e-02, -1.9221e-02,  8.7146e-03,\n",
       "         -1.4833e-03, -7.6345e-03, -7.9339e-03, -1.9430e-02,  2.4253e-02,\n",
       "         -3.8123e-02, -4.6568e-02, -1.8670e-02,  5.1211e-02, -4.2783e-03,\n",
       "         -3.8196e-04, -4.0363e-02, -9.5039e-03,  2.4656e-02, -6.1482e-02,\n",
       "         -1.7572e-02, -1.1578e-02, -2.3224e-02, -7.8378e-03,  1.7691e-02,\n",
       "          3.5012e-02,  6.3939e-03, -1.0596e-02, -2.6779e-02, -2.5244e-02,\n",
       "          5.7578e-02,  2.6650e-02, -1.0357e-02, -2.5465e-02, -4.5545e-03,\n",
       "         -2.3285e-02,  4.3090e-02,  5.6097e-03,  3.0907e-03, -8.7786e-03,\n",
       "         -8.5674e-03, -4.9801e-02,  6.6305e-03, -2.0992e-02, -2.8705e-02,\n",
       "          4.6538e-03,  3.6479e-02, -3.0568e-02,  9.6655e-03, -1.0263e-02,\n",
       "         -1.5800e-02, -2.2179e-03, -2.2886e-02,  2.0779e-02, -4.6353e-03,\n",
       "         -2.6164e-02, -1.4870e-02,  4.2060e-02, -3.1666e-02, -7.3926e-02,\n",
       "         -3.5886e-02,  2.8824e-02,  4.2069e-02,  1.8360e-02, -5.6762e-03,\n",
       "          1.3522e-02, -3.6312e-02, -3.0230e-02,  6.3013e-03,  9.2082e-03,\n",
       "          1.3700e-02, -1.6012e-02,  1.4733e-02,  2.1459e-02,  6.1978e-02,\n",
       "          4.7832e-02, -1.3874e-02,  2.8839e-02,  1.1357e-02, -5.2838e-02,\n",
       "         -4.6085e-02,  3.5132e-02, -2.0732e-02,  4.0364e-03, -6.1810e-03,\n",
       "         -1.6299e-02,  3.3830e-02, -1.9870e-03,  3.6497e-02,  2.2285e-02,\n",
       "         -3.5198e-02, -1.5771e-02, -1.1905e-02, -2.5854e-02,  1.0018e-02,\n",
       "          1.4148e-02,  1.3409e-02,  3.2050e-02,  8.1515e-03,  2.6911e-02,\n",
       "         -1.8388e-02,  6.0402e-02,  6.1563e-02,  5.0198e-03,  7.8541e-03,\n",
       "          1.2646e-02, -2.0782e-04,  3.8431e-03,  5.1785e-02,  1.5168e-34,\n",
       "         -3.1806e-02,  1.8031e-02, -1.4244e-02,  3.2452e-02,  5.3335e-02,\n",
       "         -2.8686e-02,  1.2400e-02,  1.0659e-02, -8.0498e-02,  1.3207e-02,\n",
       "         -4.8199e-02,  1.3678e-02, -6.9429e-02,  2.5634e-02,  5.5759e-02,\n",
       "         -4.9630e-02, -1.6527e-02,  2.5205e-02,  2.1564e-03,  3.4589e-02,\n",
       "          2.9639e-03,  9.4739e-03, -1.1186e-02, -3.2119e-02, -1.1241e-02,\n",
       "          5.3897e-02, -2.3779e-02,  5.0885e-03,  6.0565e-02,  9.6461e-03,\n",
       "         -5.7980e-03, -1.2712e-02, -4.6139e-02, -1.4095e-02,  8.2618e-03,\n",
       "          7.5159e-03, -1.6599e-02, -1.1664e-02, -1.7382e-02,  3.0807e-02,\n",
       "         -3.7272e-02,  1.3737e-02, -5.7255e-02, -8.8260e-03,  3.3382e-02,\n",
       "         -1.4829e-02,  2.2916e-02, -3.7202e-02, -3.3824e-02, -4.8178e-03,\n",
       "         -4.0761e-02,  1.3029e-02, -3.5435e-02,  4.4597e-02, -2.8404e-03,\n",
       "         -2.7502e-02,  4.2838e-02, -2.8727e-02, -2.6486e-02,  3.7686e-02,\n",
       "          4.5223e-02, -3.0334e-02,  2.4167e-02, -2.9788e-02, -2.0964e-02,\n",
       "          3.0055e-02,  3.2580e-02, -2.1319e-03,  7.8603e-04,  2.6279e-03,\n",
       "         -4.2180e-03, -1.3151e-02,  4.0886e-02, -8.1530e-03,  3.0161e-02,\n",
       "          1.8256e-03, -1.0796e-02, -2.8478e-02, -2.7535e-02, -4.3817e-02,\n",
       "          2.0236e-03, -1.5002e-02, -4.1846e-02,  6.8153e-03,  1.5236e-02,\n",
       "         -7.9242e-03,  1.6552e-02, -4.7653e-02, -2.4244e-02, -1.6616e-02,\n",
       "         -5.1689e-02, -3.2699e-02,  6.7178e-02,  1.3352e-02,  1.5380e-02,\n",
       "         -8.5630e-34,  1.0105e-02, -5.2413e-02, -4.3769e-02,  2.2506e-02,\n",
       "         -2.8369e-02,  1.0062e-02, -5.1694e-02,  4.6625e-02,  1.8944e-02,\n",
       "          5.3125e-02,  5.5110e-03,  3.7211e-02,  3.0795e-02,  1.3192e-02,\n",
       "          8.1663e-03, -3.6733e-02,  4.1941e-02, -2.4025e-02,  2.4410e-02,\n",
       "          4.6545e-03,  3.2766e-02,  1.4465e-02, -3.5289e-02, -4.4349e-02,\n",
       "          2.9854e-02,  3.4643e-02, -8.4534e-03, -1.4915e-02, -5.5509e-03,\n",
       "          3.6636e-02,  2.8194e-02, -2.3140e-02, -2.8487e-03, -2.4417e-02,\n",
       "          1.0471e-02,  2.4135e-02,  5.5466e-02,  3.1472e-04, -3.7510e-02,\n",
       "         -1.6312e-03,  3.1489e-02, -4.7383e-03, -3.7572e-02,  1.0403e-02,\n",
       "         -8.1180e-03,  2.7574e-02,  4.0849e-02, -1.0259e-02, -4.0972e-03,\n",
       "          3.9324e-02, -3.9364e-03, -1.5594e-02, -1.0416e-02, -3.9075e-02,\n",
       "         -1.7335e-02, -3.5363e-02, -1.2437e-02,  1.0219e-02,  1.3514e-02,\n",
       "          9.5076e-03, -2.1119e-02,  2.4249e-02, -3.8552e-02,  3.4735e-02,\n",
       "          2.9436e-02,  2.3294e-02,  1.4683e-02,  2.1775e-03, -1.1893e-02,\n",
       "         -1.3453e-02, -7.9841e-02,  1.8987e-02, -8.0614e-02,  3.8837e-02,\n",
       "          1.4049e-03, -3.9182e-03,  3.0678e-03,  2.7553e-02,  1.1968e-02,\n",
       "          2.2470e-02, -1.7148e-02, -5.3293e-03, -1.8084e-02,  1.9227e-03,\n",
       "          2.8072e-02,  2.4643e-02,  1.3463e-02,  3.1087e-02, -7.4350e-03,\n",
       "          6.1686e-04,  2.6736e-02,  4.8789e-02,  1.5627e-02,  1.8489e-03,\n",
       "          3.4892e-02, -3.2828e-08,  2.7846e-03,  5.2239e-03,  1.2833e-02,\n",
       "         -1.5013e-02,  3.1118e-03,  7.6021e-03, -1.9648e-02,  1.6627e-02,\n",
       "         -1.9692e-02,  5.1657e-02,  4.9377e-02,  2.3822e-03, -2.4493e-02,\n",
       "          4.6655e-03,  2.2749e-02,  1.3419e-02,  5.7920e-02, -1.0677e-02,\n",
       "         -1.3138e-02,  2.3902e-02,  1.2386e-02,  3.8834e-02,  2.5430e-02,\n",
       "         -8.2210e-02, -5.9448e-02,  1.5920e-02, -1.1584e-02, -2.7883e-02,\n",
       "          5.8477e-03,  1.6924e-02,  5.5870e-02,  2.1297e-02, -1.5738e-02,\n",
       "          1.3215e-02, -1.2734e-02, -2.5090e-03,  6.8100e-03,  2.2237e-02,\n",
       "          1.0581e-02, -1.8621e-03,  9.7666e-03, -1.6306e-02, -2.5785e-02,\n",
       "         -4.0631e-03, -1.1415e-02, -1.8808e-03, -2.3845e-03, -6.5954e-02,\n",
       "         -4.1797e-03, -4.7889e-02,  1.0501e-02,  1.0621e-02, -1.4222e-02,\n",
       "          7.7441e-03,  8.4079e-02,  2.5481e-02,  2.8211e-04,  4.0834e-02,\n",
       "         -3.9023e-03,  5.0439e-02,  8.1307e-02,  4.1612e-03, -6.4782e-02,\n",
       "          2.0904e-02,  0.0000e+00,  8.1529e-01,  0.0000e+00,  4.3478e-02,\n",
       "         -2.6129e-02, -5.4311e-03,  1.2576e-02, -5.7749e-02,  2.2853e-02,\n",
       "          2.4378e-02,  1.5674e-02,  9.2045e-03, -1.6154e-02,  3.0379e-02,\n",
       "         -3.1190e-02,  1.8857e-02, -2.5042e-02, -2.2335e-02, -8.7819e-03,\n",
       "          1.4422e-02,  5.0783e-02,  7.8129e-03,  2.5106e-05, -1.7170e-02,\n",
       "         -4.1233e-02, -4.9532e-02,  1.4966e-02, -1.5828e-02, -4.4520e-02,\n",
       "         -1.2176e-03, -2.7031e-02,  2.9285e-02, -1.2966e-02, -6.3275e-02,\n",
       "          8.5655e-04,  5.2901e-02,  4.8087e-03, -1.6817e-02, -3.3495e-02,\n",
       "         -1.9434e-02,  1.7831e-02, -1.9928e-02, -8.1285e-02, -1.5201e-03,\n",
       "         -4.4637e-02,  1.1604e-02,  1.2358e-02,  2.3123e-02,  1.2580e-02,\n",
       "          1.8511e-02, -1.3242e-02, -4.2619e-02,  4.3014e-02, -8.4095e-03,\n",
       "          7.6161e-03, -5.4388e-02,  3.5181e-02, -5.8221e-02,  2.3009e-02,\n",
       "          3.4149e-02,  6.9925e-04,  2.5615e-02,  2.2334e-02, -2.7897e-02,\n",
       "          1.1856e-02, -2.1955e-02, -9.1632e-03,  1.2020e-02,  5.8966e-02,\n",
       "         -5.9132e-03,  8.2510e-04, -2.8174e-02,  3.5735e-02, -3.0050e-02,\n",
       "         -1.4245e-02,  5.8171e-03,  1.7617e-02,  1.0576e-02, -1.6339e-02,\n",
       "          1.8157e-02, -3.4860e-02, -4.5709e-02, -4.8942e-02,  4.2691e-02,\n",
       "          3.2292e-02, -1.9544e-02, -3.1255e-02,  1.3667e-02,  2.0447e-02,\n",
       "         -1.9725e-02, -1.4052e-02,  1.1961e-02,  1.8706e-02,  2.7511e-03,\n",
       "         -8.5952e-05,  3.7954e-02,  7.8048e-02,  1.2774e-02,  4.2512e-03,\n",
       "          3.1856e-02,  1.8872e-02, -4.2913e-02, -5.3263e-02,  3.6599e-02,\n",
       "         -2.5891e-02, -5.1571e-04,  3.4927e-03,  2.8193e-03,  3.3388e-02,\n",
       "         -1.7004e-02,  2.3140e-02,  3.3241e-02, -1.7887e-02, -8.2124e-03,\n",
       "          3.2067e-03, -2.0367e-02,  8.5931e-04,  1.1457e-02,  6.8565e-03,\n",
       "          3.6430e-02, -2.0793e-02, -2.8719e-03, -3.4263e-03,  4.2349e-02,\n",
       "          9.4366e-02, -2.9903e-02, -3.7666e-02,  2.8396e-02,  1.6563e-02,\n",
       "         -5.2929e-03,  2.1625e-02, -1.8834e-33,  1.7933e-02,  3.7423e-03,\n",
       "         -1.1490e-02, -3.5532e-03,  7.7759e-02, -4.7990e-02,  6.3567e-03,\n",
       "          1.2127e-02, -6.7454e-02,  2.5404e-02,  1.8956e-02,  8.8276e-03,\n",
       "         -4.9630e-02,  1.8037e-02,  8.4428e-02, -7.9092e-02, -2.2138e-02,\n",
       "          2.9321e-02, -2.4660e-03,  1.3121e-02,  1.5574e-02, -7.4691e-03,\n",
       "         -1.1002e-02, -4.3459e-02,  8.4025e-03,  6.6959e-02, -3.1297e-02,\n",
       "         -3.9980e-02,  8.5037e-02,  5.2304e-03, -6.4722e-02, -4.7356e-03,\n",
       "         -6.2370e-02,  4.8314e-04,  3.6092e-03,  2.9769e-03, -9.5124e-03,\n",
       "         -2.6329e-02, -4.7549e-03,  3.3278e-02, -5.2906e-02, -3.3587e-03,\n",
       "         -3.0197e-02, -2.7519e-03,  1.1364e-02, -1.8602e-02,  4.6502e-02,\n",
       "         -3.3892e-02, -4.1023e-02, -1.3201e-03,  4.4783e-03,  2.7162e-02,\n",
       "         -1.4351e-02,  2.4904e-02,  9.4670e-03, -1.5907e-02,  1.9066e-02,\n",
       "         -1.4900e-02,  5.2408e-05,  4.4844e-02,  3.0326e-02, -3.1509e-03,\n",
       "          3.1737e-02, -1.1024e-03, -1.7714e-02,  4.9795e-02, -2.9549e-03,\n",
       "         -9.2907e-04,  2.2624e-02, -9.7993e-03,  1.6826e-03, -2.7461e-02,\n",
       "          1.8104e-02, -2.2926e-02,  6.5168e-03,  1.0263e-02,  8.3852e-03,\n",
       "         -4.2598e-02, -1.0937e-02, -4.1882e-02,  2.5387e-02, -1.9865e-02,\n",
       "         -3.0966e-02, -3.6547e-03,  2.3518e-02, -1.9783e-02,  2.3468e-02,\n",
       "         -5.1140e-02, -5.7497e-02, -1.2779e-02, -4.4696e-02, -1.3062e-02,\n",
       "          2.3053e-02,  1.6483e-02,  2.9585e-02,  1.2367e-33, -2.3993e-02,\n",
       "         -5.7208e-02, -4.1287e-02,  1.3814e-02, -6.8068e-03,  2.4247e-02,\n",
       "         -2.2269e-02,  3.2847e-02,  1.1489e-02,  5.9212e-02, -3.1862e-02,\n",
       "          5.7321e-02, -7.4077e-03,  1.7899e-02,  1.8616e-02, -3.8966e-02,\n",
       "          1.1617e-02,  3.6511e-02,  6.1608e-03,  2.7011e-03, -8.3682e-04,\n",
       "          4.2066e-02, -2.2164e-02, -3.2779e-02,  1.5197e-02,  2.7703e-02,\n",
       "         -1.7486e-02, -1.4528e-02,  1.7025e-02,  4.3640e-02,  1.2704e-02,\n",
       "          1.0922e-02, -1.0686e-02, -6.3062e-02,  3.3438e-02,  2.5636e-02,\n",
       "          2.2548e-02,  4.6548e-03, -3.6223e-02, -1.6911e-02,  8.7046e-04,\n",
       "         -1.5044e-02, -4.5117e-02, -2.0669e-02,  4.6316e-04,  3.6245e-02,\n",
       "          2.3442e-02,  3.4629e-02, -1.1009e-02,  3.6138e-02,  1.8458e-02,\n",
       "         -1.3064e-02,  1.9504e-02, -6.8516e-02, -2.1016e-02, -1.7443e-02,\n",
       "         -3.7807e-02,  1.4228e-02,  2.2443e-02,  2.4250e-02,  2.2026e-02,\n",
       "          1.9086e-02, -6.9504e-02, -1.1406e-02,  5.1501e-02,  3.6500e-03,\n",
       "         -1.4488e-02,  4.4008e-03, -2.7719e-02, -1.0721e-02, -3.1507e-02,\n",
       "          8.0598e-03, -4.3996e-02,  2.0761e-02,  2.7291e-02,  8.0318e-04,\n",
       "          1.6298e-02,  1.4302e-02,  1.4694e-02, -3.9969e-03, -1.1635e-02,\n",
       "          4.4530e-03,  3.0539e-02, -2.1601e-02, -9.7971e-03,  3.4690e-02,\n",
       "         -2.8079e-02,  3.1029e-02, -1.7595e-02,  6.3082e-03,  3.5875e-02,\n",
       "          5.2201e-02,  4.5324e-02,  6.1677e-02,  6.3671e-03, -4.0178e-08,\n",
       "          2.0801e-02,  4.3510e-03,  3.4572e-02, -1.4489e-02, -1.3424e-03,\n",
       "          9.6224e-03,  1.7624e-02, -7.3814e-03, -2.4651e-02,  4.5157e-02,\n",
       "          1.1206e-02, -3.1623e-02, -3.0125e-02,  2.7484e-02, -2.5753e-04,\n",
       "          4.0489e-02,  3.0136e-02,  7.1409e-03, -4.2280e-02,  5.3549e-02,\n",
       "          2.5400e-02,  5.8157e-02, -9.6657e-03, -4.8662e-02, -8.2642e-02,\n",
       "          1.9338e-03, -8.9419e-04, -3.8166e-03,  3.4261e-02,  8.2694e-03,\n",
       "          9.4404e-03,  3.0816e-02,  1.7264e-02,  6.2865e-02, -1.7614e-03,\n",
       "          3.9868e-03, -1.6564e-02,  2.7341e-02, -2.9847e-02, -9.7295e-03,\n",
       "         -2.1319e-02,  3.6959e-03, -2.0152e-04,  8.7051e-03, -5.4278e-02,\n",
       "         -1.1621e-02, -6.9743e-03, -8.0731e-02, -7.1537e-03, -7.5727e-03,\n",
       "          5.3322e-03,  1.1250e-02, -1.8800e-02,  2.4134e-03,  1.0191e-01,\n",
       "          4.0839e-02, -7.2142e-03,  3.1863e-02, -1.2989e-02,  2.1061e-02,\n",
       "          3.1713e-02,  1.7267e-02, -3.8095e-02,  4.4375e-02]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PlayedDataset(Dataset):\n",
    "    def __init__(self, df) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        userID = row['userID']\n",
    "        itemID = row['itemID']\n",
    "        userIDX = row['userIDX']\n",
    "        itemIDX = row['itemIDX']\n",
    "        negaID = random.choice(tuple(itemset.difference(I[userID]))) # Negative item ID\n",
    "        negaIDX = item_oe.transform([[negaID]])[0][0]\n",
    "\n",
    "        # Build positive pair\n",
    "        pos = np.concatenate((row[2:].to_numpy().astype(np.float32),\n",
    "                              user_mean.iloc[userIDX].to_numpy().astype(np.float32),\n",
    "                              item_mean.iloc[itemIDX].to_numpy().astype(np.float32)))\n",
    "        negrow = row.copy()\n",
    "        negrow['userIDX'] = negaIDX\n",
    "        neg = np.concatenate((negrow[2:].to_numpy().astype(np.float32),\n",
    "                              user_mean.iloc[userIDX].to_numpy().astype(np.float32),\n",
    "                              item_mean.iloc[negaIDX].to_numpy().astype(np.float32)))\n",
    "        return torch.from_numpy(pos).to(dtype=torch.float32), torch.from_numpy(neg).to(dtype=torch.float32)\n",
    "\n",
    "played_ds = PlayedDataset(df_played_train)\n",
    "played_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_feature, latent_dim) -> None:\n",
    "        \"\"\"\n",
    "        n_user: Number of unique users\n",
    "        n_item: Number of unique items\n",
    "        n_feature: Number of extra features to use\n",
    "        latent_dim: Dimension of latent representations of users/items/features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.n_feature = n_feature\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.user_latent = nn.Embedding(n_user, latent_dim)\n",
    "        self.item_latent = nn.Embedding(n_item, latent_dim)\n",
    "        self.feat_latent = nn.Embedding(n_feature, latent_dim)\n",
    "        self.user_weight = nn.Embedding(n_user, 1)\n",
    "        self.item_weight = nn.Embedding(n_item, 1)\n",
    "        # \"alpha\" or \"w_0\" term will be absorbed into feat_weight linear's bias\n",
    "        self.feat_weight = nn.Linear(n_feature, 1)\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input shape: batch_size x (user idx, item idx, features) - 2 dimensional\n",
    "        Returns: n x 1 tensor of predictions\n",
    "        \"\"\"\n",
    "        if len(x.size()) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        # f(u, i) = w_0 + \\sum_{j=1}^{d} w_j * x_j\n",
    "        out = self.feat_weight(x[:, 2:])\n",
    "        users = x[:, 0].to(dtype=torch.int32)\n",
    "        items = x[:, 1].to(dtype=torch.int32)\n",
    "        out += self.user_weight(users)\n",
    "        out += self.item_weight(items)\n",
    "        # Nested summation thingy\n",
    "        # Interactions between users/items and features\n",
    "        u_embed = self.user_latent(users)\n",
    "        i_embed = self.item_latent(items)\n",
    "        f_embed = self.feat_latent(torch.Tensor(range(0, self.n_feature)).to(device, dtype=torch.int32))\n",
    "        out += (u_embed * i_embed).sum(dim=1).unsqueeze(-1)   # Dot product between user and item latent representations\n",
    "        out += (u_embed @ f_embed.T).sum(dim=1).unsqueeze(-1) # Dot product between user and feature latent representations\n",
    "        out += (i_embed @ f_embed.T).sum(dim=1).unsqueeze(-1) # Dot product between item and feature latent representations\n",
    "        # Interactions between features\n",
    "        for i in range(0, self.n_feature):\n",
    "            for j in range(0, i):\n",
    "                out += (f_embed[i] @ f_embed[j].T) * (x[:, 2 + i] * x[:, 2 + j]).unsqueeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_model = FactorizationMachine(len(df['userID'].unique()), len(df['itemID'].unique()), 792, 16).to(device)\n",
    "played_dl = DataLoader(dataset=played_ds,\n",
    "                       batch_size=20,\n",
    "                       num_workers=4)\n",
    "optimizer = optim.RMSprop(played_model.parameters(), lr=0.001, weight_decay=0.2)\n",
    "def BPRLoss(pos_score, neg_score):\n",
    "     return -(pos_score - neg_score).sigmoid().log().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbxia/.local/lib/python3.9/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but OrdinalEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3259/1336263295.py:46: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  out += (f_embed[i] @ f_embed[j].T) * (x[:, 2 + i] * x[:, 2 + j]).unsqueeze(-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2303.8462]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "played_model(played_ds[0][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/7500\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "for i, (pos, neg) in enumerate(played_dl):\n",
    "    print(f\"{i}/{len(played_dl)}\")\n",
    "    pos_score = played_model(pos.to(device))\n",
    "    neg_score = played_model(neg.to(device))\n",
    "    loss = BPRLoss(pos_score, neg_score)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"ACU: {torch.sum(pos_score > neg_score) / len(pos_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

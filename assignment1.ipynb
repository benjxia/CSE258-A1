{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from rankfm.rankfm import RankFM\n",
    "from fastFM import als, sgd\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [],
   "source": [
    "user_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5, handle_unknown='use_encoded_value', unknown_value=6710)\n",
    "item_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "\n",
    "itemset = set() # Set of all unique users\n",
    "userset = set() # Set of all unique items\n",
    "U = defaultdict(set)\n",
    "I = defaultdict(set)\n",
    "time_played = defaultdict(dict)\n",
    "item_mean_hr = defaultdict()\n",
    "user_mean_hr = defaultdict()\n",
    "ft = ['early_access', 'compensation'] # features unavailable/cannot be approximated in inference\n",
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data():\n",
    "    global itemset, userset, U, I, user_mean_hr, item_mean_hr\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "        time_played[entry['userID']][entry['gameID']] = entry['hours_transformed']\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "    del data\n",
    "    itemset = set(df['gameID'].unique())\n",
    "    userset = set(df['userID'].unique())\n",
    "\n",
    "    U = dict(df.groupby('gameID')['userID'].unique())\n",
    "    I = dict(df.groupby('userID')['gameID'].unique())\n",
    "    U = { g : set(U[g]) for g in U }\n",
    "    I = { u : set(I[u]) for u in I }\n",
    "\n",
    "    df['userIDX'] = user_oe.fit_transform(df[['userID']])\n",
    "    df['itemIDX'] = item_oe.fit_transform(df[['gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "    # Get features that won't be available\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "\n",
    "    time_label = df['hours_transformed']\n",
    "    item_mean_hr = dict(df.groupby('itemID')['hours_transformed'].mean())\n",
    "    user_mean_hr = dict(df.groupby('userID')['hours_transformed'].mean())\n",
    "    return df, time_label\n",
    "\n",
    "df, time_label = process_data()\n",
    "user_mean_ft = df.groupby('userIDX')[ft].mean()\n",
    "item_mean_ft = df.groupby('itemIDX')[ft].mean()\n",
    "df.drop(labels=ft + ['hours_transformed', 'found_funny'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustoi = dict(df.groupby('userID')['userIDX'].unique().apply(lambda x: x[0]))\n",
    "istoi = dict(df.groupby('itemID')['itemIDX'].unique().apply(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        dftext = df.groupby('itemIDX')['text'].apply(' '.join).reset_index()\n",
    "        counter = feature_extraction.text.CountVectorizer(min_df=0.05, max_df=0.5, stop_words='english', max_features=2000, ngram_range=(1, 2))\n",
    "        wordcount = counter.fit_transform(dftext['text'])\n",
    "        LDA = LatentDirichletAllocation(n_components=20, random_state=RANDOM_SEED)\n",
    "        text_embed = LDA.fit_transform(wordcount)\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "text_embed = get_text_embedding()\n",
    "# text_embed = text_embed / np.linalg.norm(text_embed, axis=1)[...,None]\n",
    "\n",
    "df.drop('text', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = np.concatenate((np.arange(0, len(text_embed))[:,  None], text_embed, item_mean_ft.to_numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:150000]\n",
    "df_time_train_label = time_label[:150000]\n",
    "df_valid = df.iloc[150000:]\n",
    "df_time_valid_label = time_label[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_model = RankFM(factors=5,\n",
    "               loss='warp',\n",
    "               max_samples=300,\n",
    "               learning_exponent=0.25,\n",
    "               learning_schedule='invscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new validation set w/ negative pairs\n",
    "neg_pairs = []\n",
    "for review in df_valid.iterrows():\n",
    "    review = review[1]\n",
    "    sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "    neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "pos_pairs = df_valid[['userIDX', 'itemIDX']].to_numpy()\n",
    "neg_pairs = np.array(neg_pairs)\n",
    "\n",
    "def played_validate(model):\n",
    "    pos_scores = model.predict(pos_pairs)\n",
    "    neg_scores = model.predict(neg_pairs)\n",
    "    acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "    print(f'Validation %: {acc * 100}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation stuff - determine factor dimensions\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# if not test:\n",
    "#     kf = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "#     for k in [1, 2, 3, 4, 5, 6, 10, 20]:\n",
    "#         played_model = RankFM(factors=k,\n",
    "#                     loss='warp',\n",
    "#                     max_samples=300,\n",
    "#                     learning_exponent=0.25,\n",
    "#                     learning_schedule='invscaling')\n",
    "#         fold_accs = []\n",
    "#         for i, (train, test) in enumerate(kf.split(df[['userIDX', 'itemIDX']])):\n",
    "#             played_model.fit(df.iloc[train][['userIDX', 'itemIDX']], item_features=text_embed, epochs=20, verbose=False)\n",
    "#             neg_pairs = []\n",
    "#             for review in df.iloc[test].iterrows():\n",
    "#                 review = review[1]\n",
    "#                 sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "#                 neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "#             pos_pairs = df.iloc[test][['userIDX', 'itemIDX']].to_numpy()\n",
    "#             neg_pairs = np.array(neg_pairs)\n",
    "#             pos_scores = played_model.predict(pos_pairs)\n",
    "#             neg_scores = played_model.predict(neg_pairs)\n",
    "#             acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "#             fold_accs.append(acc)\n",
    "#             print(f'Validation %: {acc * 100}')\n",
    "#         print(f'k: {k} = {np.mean(fold_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine training epochs\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "# if not test:\n",
    "#     accs = np.zeros((10, 50))\n",
    "#     for j, (train, test) in enumerate(kf.split(df[['userIDX', 'itemIDX']])):\n",
    "#         played_model = RankFM(factors=5,\n",
    "#                 loss='warp',\n",
    "#                 max_samples=300,\n",
    "#                 learning_exponent=0.25,\n",
    "#                 learning_schedule='invscaling')\n",
    "#         for i in range(50):\n",
    "#             played_model.fit_partial(df.iloc[train][['userIDX', 'itemIDX']], item_features=text_embed, epochs=4, verbose=False)\n",
    "#             neg_pairs = []\n",
    "#             for review in df.iloc[test].iterrows():\n",
    "#                 review = review[1]\n",
    "#                 sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "#                 neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "#             pos_pairs = df.iloc[test][['userIDX', 'itemIDX']].to_numpy()\n",
    "#             neg_pairs = np.array(neg_pairs)\n",
    "#             pos_scores = played_model.predict(pos_pairs)\n",
    "#             neg_scores = played_model.predict(neg_pairs)\n",
    "#             acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "#             print(f'Validation %: {acc * 100}')\n",
    "#             accs[j, i] = acc\n",
    "\n",
    "#     print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factors=10 ~.71.5\n",
    "factors=5  ~.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "save = False\n",
    "played_model = RankFM(factors=5,\n",
    "        loss='warp',\n",
    "        max_samples=300,\n",
    "        learning_exponent=0.25,\n",
    "        learning_schedule='invscaling')\n",
    "if train == True:\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    for i in range(50):\n",
    "        # switch fit_partial's dataframe to df_train for testing, \"df\" for actual predictions\n",
    "        if test == True:\n",
    "            played_model.fit_partial(df[['userIDX', 'itemIDX']], item_features=text_embed, epochs=4, verbose=False)\n",
    "        else:\n",
    "            played_model.fit_partial(df_time_train_label[['userIDX', 'itemIDX']], item_features=text_embed, epochs=4, verbose=False)\n",
    "        acc = played_validate(played_model)\n",
    "        if acc > best_acc:\n",
    "            best_model = copy.deepcopy(played_model)\n",
    "            best_acc = acc\n",
    "    if save == True:\n",
    "        model_file = open('rankfm.obj', 'wb')\n",
    "        pickle.dump(best_model, model_file)\n",
    "        model_file.close()\n",
    "else:\n",
    "    model_file = open('rankfm.obj', 'rb')\n",
    "    best_model = pickle.load(model_file)\n",
    "    played_model = best_model\n",
    "    model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred = test.copy()\n",
    "# test['itemID'] = test['gameID']\n",
    "# # Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "# test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "# test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "# test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "# test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "# scores = best_model.predict(test[['userIDX', 'itemIDX']])\n",
    "# testpred = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred['prediction'] = (scores >= np.median(scores)).astype(np.int32)\n",
    "# testpred.to_csv('./predictions_Played.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6698, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df: pd.DataFrame):\n",
    "    datum = np.zeros((len(df), 10 + 10 + 22))\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        user = row['userIDX']\n",
    "        item = row['itemIDX']\n",
    "        datum[i, :10] = played_model.v_u[user]\n",
    "        datum[i, 10:20] = played_model.v_i[item]\n",
    "        datum[i, 20:] = text_embed[item, 1:]\n",
    "    return datum\n",
    "time_train = convert_df(df_train)\n",
    "time_valid = convert_df(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering with played prediction latent factors (this sucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1271098722319834\n"
     ]
    }
   ],
   "source": [
    "def lr_sim(item_i, item_j):\n",
    "    lr_item_i = best_model.v_i[item_i]\n",
    "    lr_item_j = best_model.v_i[item_j]\n",
    "    return np.dot(lr_item_i, lr_item_j) / (np.linalg.norm(lr_item_i) * np.linalg.norm(lr_item_j))\n",
    "\n",
    "def jaccard_sim(item_i, item_j):\n",
    "    s1 = U[item_i]\n",
    "    s2 = U[item_j]\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "def cf_predict(user_id, user_idx, item_id, item_idx):\n",
    "    sim_sum = 0 # Sum of similarity scores (besides current)\n",
    "    output = 0\n",
    "    for item_j in time_played[user_id]:\n",
    "        if item_j == item_id:\n",
    "            continue\n",
    "        sim = lr_sim(item_idx, istoi[item_j])\n",
    "        # sim = jaccard_sim(item_j, item_id)\n",
    "        score = sim * (time_played[user_id][item_j] - item_mean_hr[item_j])\n",
    "        output += score\n",
    "        sim_sum += np.abs(sim)\n",
    "    if sim_sum == 0:\n",
    "        return item_mean_hr[item_id]\n",
    "    output /= sim_sum\n",
    "    output += item_mean_hr[item_id]\n",
    "    return output\n",
    "\n",
    "# preds = np.zeros((len(df_train)))\n",
    "# for i in range(len(df_train)):\n",
    "#     row = df_train.iloc[i]\n",
    "#     preds[i] = cf_predict(row['userID'], row['userIDX'], row['itemID'], row['itemIDX'])\n",
    "# print(np.mean((preds - df_time_train_label)**2))\n",
    "# preds = np.zeros((len(df_time_valid_label)))\n",
    "# for i in range(len(df_valid)):\n",
    "#     row = df_valid.iloc[i]\n",
    "#     preds[i] = cf_predict(row['userID'], row['userIDX'], row['itemID'], row['itemIDX'])\n",
    "# print(np.mean((preds - df_time_valid_label)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with played predictioin latent factors (this sucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9195459794186234\n",
      "3.1357034540118587\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn import ensemble\n",
    "# time_model = xgboost.XGBRegressor(n_estimators=100, reg_alpha=1, gamma=5, max_depth=5)\n",
    "# # time_model = ensemble.RandomForestRegressor(n_estimators=10, max_depth=10, max_features='sqrt', n_jobs=-1)\n",
    "# time_model.fit(time_train, df_time_train_label)\n",
    "\n",
    "# train_preds = time_model.predict(time_train)\n",
    "# # train_preds[train_preds < 0] = 0\n",
    "# # train_preds[train_preds > 14] = 14\n",
    "# print(np.mean((train_preds - df_time_train_label)**2))\n",
    "# valid_preds = time_model.predict(time_valid)\n",
    "# # valid_preds[valid_preds < 0] = 0\n",
    "# # valid_preds[valid_preds > 14] = 14\n",
    "# MSE = np.mean((valid_preds - df_time_valid_label)**2)\n",
    "# print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastFM,(this sucks but not as much, with or without features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7433401715394616\n",
      "3.05446116469164\n"
     ]
    }
   ],
   "source": [
    "# time_model = als.FMRegression(n_iter=1,\n",
    "#                               rank=3,\n",
    "#                               random_state=RANDOM_SEED,\n",
    "#                               l2_reg=0.01)\n",
    "\n",
    "# df_train.head()\n",
    "# def convert_sparse_df(df: pd.DataFrame):\n",
    "#     datum = sparse.lil_matrix((len(df), len(itemset) + len(userset) + 22))\n",
    "#     for i, (idx, row) in enumerate(df.iterrows()):\n",
    "#         user = row['userIDX']\n",
    "#         item = row['itemIDX']\n",
    "#         datum[i, user] = 1\n",
    "#         datum[i, len(userset) + item] = 1\n",
    "#         datum[i, len(userset) + len(itemset):] = text_embed[item, 1:]\n",
    "#     return datum\n",
    "\n",
    "# time_train = convert_sparse_df(df_train)\n",
    "# time_valid = convert_sparse_df(df_valid)\n",
    "# time_model = als.FMRegression(n_iter=1000,\n",
    "#                               rank=0,\n",
    "#                               init_stdev=0.001,\n",
    "#                               random_state=RANDOM_SEED,\n",
    "#                               l2_reg_w=5,\n",
    "#                               l2_reg_V=2)\n",
    "\n",
    "# time_model.fit(time_train, df_time_train_label)\n",
    "# train_preds = time_model.predict(time_train)\n",
    "# # train_preds[train_preds < 0] = 0\n",
    "# # train_preds[train_preds > 14] = 14\n",
    "# print(np.mean((train_preds - df_time_train_label)**2))\n",
    "# valid_preds = time_model.predict(time_valid)\n",
    "# # valid_preds[valid_preds < 0] = 0\n",
    "# # valid_preds[valid_preds > 14] = 14\n",
    "# MSE = np.mean((valid_preds - df_time_valid_label)**2)\n",
    "# print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (363181243.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[106], line 32\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i in tqdm(range(300)):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "beta_u = np.zeros(len(I))\n",
    "beta_i = np.zeros(len(U))\n",
    "alpha = time_label.mean()\n",
    "u_cnts = df['userIDX'].value_counts()\n",
    "i_cnts = df['itemIDX'].value_counts()\n",
    "def closed_form(lamb, trainset, trainlabel):\n",
    "    global alpha\n",
    "    global beta_u\n",
    "    global beta_i\n",
    "    trainset = trainset.copy()\n",
    "    user_indices = trainset['userIDX'].tolist()\n",
    "    item_indices = trainset['itemIDX'].tolist()\n",
    "    trainset['label'] = trainlabel\n",
    "    labelsum = trainset.groupby('userIDX')['label'].sum().tolist()\n",
    "    usersum = trainset.groupby('userIDX')['itemIDX'].tolist()\n",
    "    alpha = np.mean(trainlabel.to_numpy() - beta_u[user_indices] - beta_i[item_indices])\n",
    "    for u in range(len(beta_u)):\n",
    "        beta_u[u] = trainsum[u] - (alpha * u_cnts[u]) - beta_i[]\n",
    "    # beta_u[user_indices] = trainlabel[]\n",
    "    # for i in range(len(trainset)):\n",
    "    #     row = trainset.iloc[i]\n",
    "    #     user = row['userIDX']\n",
    "    #     item = row['itemIDX']\n",
    "    #     new_beta_u[user] += (trainlabel[i] - alpha - beta_i[item]) / (lamb  + u_cnts[user])\n",
    "    # beta_u = new_beta_u\n",
    "\n",
    "    # for i in range(len(trainset)):\n",
    "    #     row = trainset.iloc[i]\n",
    "    #     user = row['userIDX']\n",
    "    #     item = row['itemIDX']\n",
    "    #     new_beta_i[item] += (trainlabel[i] - alpha - beta_u[user]) / (lamb + i_cnts[item])\n",
    "    # beta_i = new_beta_i\n",
    "\n",
    "for i in tqdm(range(300)):\n",
    "    closed_form(5, df_train, df_time_train_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userIDX  itemIDX\n",
       "0        21         1\n",
       "         169        1\n",
       "         292        1\n",
       "         710        1\n",
       "         974        1\n",
       "                   ..\n",
       "6697     1765       1\n",
       "         1782       1\n",
       "         1944       1\n",
       "         2183       1\n",
       "         2242       1\n",
       "Name: itemIDX, Length: 149999, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('userIDX')['itemIDX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred = test.copy()\n",
    "test['itemID'] = test['gameID']\n",
    "# Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "\n",
    "time_test = convert_df(test)\n",
    "preds = time_model.predict(time_test)\n",
    "\n",
    "testpred = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred['prediction'] = preds\n",
    "testpred.to_csv('./predictions_Hours.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

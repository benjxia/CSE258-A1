{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from rankfm.rankfm import RankFM\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [],
   "source": [
    "user_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5, handle_unknown='use_encoded_value', unknown_value=6710)\n",
    "item_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "\n",
    "itemset = set() # Set of all unique users\n",
    "userset = set() # Set of all unique items\n",
    "U = defaultdict(set)\n",
    "I = defaultdict(set)\n",
    "\n",
    "ft = ['early_access', 'compensation'] # features unavailable/cannot be approximated in inference\n",
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data():\n",
    "    global itemset, userset, U, I\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "    del data\n",
    "    itemset = set(df['gameID'].unique())\n",
    "    userset = set(df['userID'].unique())\n",
    "\n",
    "    U = dict(df.groupby('gameID')['userID'].unique())\n",
    "    I = dict(df.groupby('userID')['gameID'].unique())\n",
    "    U = { g : set(U[g]) for g in U }\n",
    "    I = { u : set(I[u]) for u in I }\n",
    "\n",
    "    df['userIDX'] = user_oe.fit_transform(df[['userID']])\n",
    "    df['itemIDX'] = item_oe.fit_transform(df[['gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Get features that won't be available\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "\n",
    "    time_label = df['hours_transformed']\n",
    "\n",
    "    return df, time_label\n",
    "\n",
    "df, time_label = process_data()\n",
    "user_mean = df.groupby('userIDX')[ft].mean()\n",
    "item_mean = df.groupby('itemIDX')[ft].mean()\n",
    "df.drop(labels=ft + ['hours_transformed', 'found_funny'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustoi = dict(df.groupby('userID')['userIDX'].unique().apply(lambda x: x[0]))\n",
    "istoi = dict(df.groupby('itemID')['itemIDX'].unique().apply(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        dftext = df.groupby('itemIDX')['text'].apply(' '.join).reset_index()\n",
    "        counter = feature_extraction.text.CountVectorizer(min_df=0.05, max_df=0.5, stop_words='english', max_features=2000, ngram_range=(1, 2))\n",
    "        wordcount = counter.fit_transform(dftext['text'])\n",
    "        LDA = LatentDirichletAllocation(n_components=20, random_state=RANDOM_SEED)\n",
    "        text_embed = LDA.fit_transform(wordcount)\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "text_embed = get_text_embedding()\n",
    "text_embed = text_embed / np.linalg.norm(text_embed, axis=1)[...,None]\n",
    "\n",
    "df.drop('text', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = np.concatenate((np.arange(0, len(text_embed))[:,  None], text_embed, item_mean.to_numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:150000]\n",
    "df_time_train_label = time_label[:150000]\n",
    "df_valid = df.iloc[150000:]\n",
    "df_time_valid_label = time_label[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_model = RankFM(factors=10,\n",
    "               loss='warp',\n",
    "               max_samples=300,\n",
    "               learning_exponent=0.25,\n",
    "               learning_schedule='invscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new validation set w/ negative pairs\n",
    "neg_pairs = []\n",
    "for review in df_valid.iterrows():\n",
    "    review = review[1]\n",
    "    sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "    neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "pos_pairs = df_valid[['userIDX', 'itemIDX']].to_numpy()\n",
    "neg_pairs = np.array(neg_pairs)\n",
    "\n",
    "def played_validate(model):\n",
    "    pos_scores = model.predict(pos_pairs)\n",
    "    neg_scores = model.predict(neg_pairs)\n",
    "    acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "    print(f'Validation %: {acc * 100}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = None\n",
    "# best_acc = 0\n",
    "# for i in range(50):\n",
    "#     played_model.fit_partial(df_train[['userIDX', 'itemIDX']], item_features=text_embed, epochs=4, verbose=False)\n",
    "#     acc = played_validate(played_model)\n",
    "#     if acc > best_acc:\n",
    "#         best_model = copy.deepcopy(played_model)\n",
    "#         best_acc = acc\n",
    "\n",
    "# model_file = open('rankfm.obj', 'wb')\n",
    "# pickle.dump(best_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred = test.copy()\n",
    "# test['itemID'] = test['gameID']\n",
    "# # Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "# test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "# test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "# test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "# test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "# scores = best_model.predict(test[['userIDX', 'itemIDX']])\n",
    "# testpred = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred['prediction'] = (scores >= 0).astype(np.int32)\n",
    "# testpred.to_csv('./predictions_Played.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.280727226141243"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlabel = df_time_train_label.to_numpy()\n",
    "mean = np.mean(tlabel)\n",
    "np.mean((tlabel - mean)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, df, label) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        itemIDX = row['itemIDX']\n",
    "\n",
    "        # Build positive pair\n",
    "        data = np.concatenate((row[2:].to_numpy().astype(np.float32),\n",
    "                              text_embed[itemIDX][1:].astype(np.float32)))\n",
    "        label = self.label[index]\n",
    "        return torch.from_numpy(data).to(dtype=torch.float64), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    class User_Param(nn.Module):\n",
    "        pass\n",
    "    def __init__(self, n_user, n_item, n_feature, latent_dim, weight=True) -> None:\n",
    "        \"\"\"\n",
    "        n_user: Number of unique users\n",
    "        n_item: Number of unique items\n",
    "        n_feature: Number of extra features to use\n",
    "        latent_dim: Dimension of latent representations of users/items/features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.n_feature = n_feature\n",
    "        self.latent_dim = latent_dim\n",
    "        self.weight = weight\n",
    "        self.user_latent = nn.Embedding(n_user, latent_dim)\n",
    "        self.item_latent = nn.Embedding(n_item, latent_dim)\n",
    "        self.feat_latent = nn.Parameter(torch.randn(n_feature, latent_dim), requires_grad=True)\n",
    "        self.feat_weight = nn.Linear(n_feature, 1)\n",
    "        self.user_weight = nn.Embedding(n_user, 1)\n",
    "        self.item_weight = nn.Embedding(n_item, 1)\n",
    "        # \"alpha\" or \"w_0\" term will be absorbed into feat_weight linear's bias\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input shape: batch_size x (user idx, item idx, features) - 2 dimensional\n",
    "        Returns: n x 1 tensor of predictions\n",
    "        \"\"\"\n",
    "        if len(x.size()) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        # f(u, i) = w_0 + \\sum_{j=1}^{d} w_j * x_j\n",
    "        out = torch.zeros((x.size()[0], 1), device=device)\n",
    "        if self.n_feature > 0 and self.weight:\n",
    "            out += self.feat_weight(x[:, 2:])\n",
    "        users = x[:, 0].to(dtype=torch.int32)\n",
    "        items = x[:, 1].to(dtype=torch.int32)\n",
    "        if self.weight:\n",
    "            out += self.user_weight(users)\n",
    "            out += self.item_weight(items)\n",
    "        # Nested summation thingy\n",
    "        # Interactions between users/items and features\n",
    "        u_embed = self.user_latent(users)\n",
    "        i_embed = self.item_latent(items)\n",
    "        out += (u_embed * i_embed).sum(dim=1).unsqueeze(-1)   # Dot product between user and item latent representations\n",
    "        if self.n_feature > 0:\n",
    "            # Interactions between features\n",
    "            xfeature = x[:, 2:]\n",
    "            out += ((u_embed @ self.feat_latent.T) * xfeature).sum(dim=1).unsqueeze(-1) # Dot product between user and feature latent representations\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_model = FactorizationMachine(len(df['userID'].unique()), len(df['itemID'].unique()), 22, 5, True).to(device) # 24 features\n",
    "batch_sz=10\n",
    "print_iter=1000\n",
    "time_train_ds = TimeDataset(df_train, df_time_train_label)\n",
    "time_train_dl = DataLoader(dataset=time_train_ds,\n",
    "                       batch_size=batch_sz,\n",
    "                       shuffle=True, num_workers=2)\n",
    "time_valid_ds = TimeDataset(df_valid.reset_index(drop=True), df_time_valid_label.reset_index(drop=True))\n",
    "time_valid_dl = DataLoader(dataset=time_valid_ds,\n",
    "                             batch_size=1,\n",
    "                             num_workers=2)\n",
    "\n",
    "def criterion(pred, label):\n",
    "    return torch.mean((pred - label)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "user_optimizers = [optim.SGD(time_model.user_latent.parameters(), lr=lr),\n",
    "                   optim.SGD(time_model.user_weight.parameters(), lr=lr)]\n",
    "item_optimizers = [optim.SGD(time_model.feat_weight.parameters(), lr=lr),\n",
    "                   optim.SGD([{'params' : time_model.feat_latent }], lr=lr),\n",
    "                   optim.SGD(time_model.item_weight.parameters(), lr=lr),\n",
    "                   optim.SGD(time_model.item_latent.parameters(), lr=lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [01:19, 188.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.764434335158103\n"
     ]
    }
   ],
   "source": [
    "def time_validate(model):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i in range(len(time_valid_ds)):\n",
    "            data, label = time_valid_ds[i]\n",
    "            pred = model(data.to(device))\n",
    "            loss += criterion(pred, label)\n",
    "        return (loss / len(time_valid_ds)).item()\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "def train_epoch(optimizers):\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, (data, label) in tqdm(enumerate(time_train_dl)):\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "        preds = time_model(data.to(device))\n",
    "        loss = criterion(preds, label)\n",
    "        running_loss += loss\n",
    "        loss.backward()\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "\n",
    "    print((running_loss / (len(df_time_train_label) / batch_sz)).item())\n",
    "    print(f'Validation MSE: {time_validate(time_model)}')\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    if i % 2 == 0:\n",
    "        train_epoch(user_optimizers)\n",
    "    else:\n",
    "        train_epoch(item_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred = test.copy()\n",
    "test['itemID'] = test['gameID']\n",
    "# Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(test)):\n",
    "    row = test.iloc[i]\n",
    "    itemIDX = row['itemIDX']\n",
    "    # Build positive pair\n",
    "    data = np.concatenate((row[2:].to_numpy().astype(np.float64),\n",
    "                            text_embed[itemIDX][1:].astype(np.float64)))\n",
    "    preds.append(time_model(torch.tensor(data).to(device)).item())\n",
    "\n",
    "\n",
    "testpred = pd.read_csv('./pairs_Hours.csv')\n",
    "testpred['prediction'] = preds\n",
    "testpred.to_csv('./predictions_Hours.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

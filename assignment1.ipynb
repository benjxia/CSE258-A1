{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from rankfm.rankfm import RankFM\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [],
   "source": [
    "user_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5, handle_unknown='use_encoded_value', unknown_value=6710)\n",
    "item_oe = preprocessing.OrdinalEncoder(dtype=np.int32, min_frequency=5)\n",
    "\n",
    "itemset = set() # Set of all unique users\n",
    "userset = set() # Set of all unique items\n",
    "U = defaultdict(set)\n",
    "I = defaultdict(set)\n",
    "\n",
    "ft = ['early_access', 'compensation'] # features unavailable/cannot be approximated in inference\n",
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data():\n",
    "    global itemset, userset, U, I\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "    del data\n",
    "    itemset = set(df['gameID'].unique())\n",
    "    userset = set(df['userID'].unique())\n",
    "\n",
    "    U = dict(df.groupby('gameID')['userID'].unique())\n",
    "    I = dict(df.groupby('userID')['gameID'].unique())\n",
    "    U = { g : set(U[g]) for g in U }\n",
    "    I = { u : set(I[u]) for u in I }\n",
    "\n",
    "    df['userIDX'] = user_oe.fit_transform(df[['userID']])\n",
    "    df['itemIDX'] = item_oe.fit_transform(df[['gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Get features that won't be available\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "\n",
    "    time_label = df['hours_transformed']\n",
    "\n",
    "    return df, time_label\n",
    "\n",
    "df, time_label = process_data()\n",
    "user_mean = df.groupby('userIDX')[ft].mean()\n",
    "item_mean = df.groupby('itemIDX')[ft].mean()\n",
    "df.drop(labels=ft + ['hours_transformed', 'found_funny'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustoi = dict(df.groupby('userID')['userIDX'].unique().apply(lambda x: x[0]))\n",
    "istoi = dict(df.groupby('itemID')['itemIDX'].unique().apply(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        dftext = df.groupby('itemIDX')['text'].apply(' '.join).reset_index()\n",
    "        counter = feature_extraction.text.CountVectorizer(min_df=0.05, max_df=0.5, stop_words='english', max_features=2000, ngram_range=(1, 2))\n",
    "        wordcount = counter.fit_transform(dftext['text'])\n",
    "        LDA = LatentDirichletAllocation(n_components=20, random_state=RANDOM_SEED)\n",
    "        text_embed = LDA.fit_transform(wordcount)\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "text_embed = get_text_embedding()\n",
    "text_embed = text_embed / np.linalg.norm(text_embed, axis=1)[...,None]\n",
    "\n",
    "df.drop('text', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = np.concatenate((np.arange(0, len(text_embed))[:,  None], text_embed, item_mean.to_numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:150000]\n",
    "df_time_train_label = time_label[:150000]\n",
    "df_valid = df.iloc[150000:]\n",
    "df_time_valid_label = time_label[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_model = RankFM(factors=10,\n",
    "               loss='warp',\n",
    "               max_samples=300,\n",
    "               learning_exponent=0.25,\n",
    "               learning_schedule='invscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new validation set w/ negative pairs\n",
    "neg_pairs = []\n",
    "for review in df_valid.iterrows():\n",
    "    review = review[1]\n",
    "    sample = random.sample(itemset.difference(I[review['userID']]), k=1)[0]\n",
    "    neg_pairs.append([review['userIDX'], istoi[sample]])\n",
    "pos_pairs = df_valid[['userIDX', 'itemIDX']].to_numpy()\n",
    "neg_pairs = np.array(neg_pairs)\n",
    "\n",
    "def played_validate(model):\n",
    "    pos_scores = model.predict(pos_pairs)\n",
    "    neg_scores = model.predict(neg_pairs)\n",
    "    acc = (np.mean(pos_scores >= 0) + np.mean(neg_scores < 0)) / 2\n",
    "    print(f'Validation %: {acc * 100}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = None\n",
    "# best_acc = 0\n",
    "# for i in range(50):\n",
    "#     played_model.fit_partial(df_train[['userIDX', 'itemIDX']], item_features=text_embed, epochs=4, verbose=False)\n",
    "#     acc = played_validate(played_model)\n",
    "#     if acc > best_acc:\n",
    "#         best_model = copy.deepcopy(played_model)\n",
    "#         best_acc = acc\n",
    "\n",
    "# model_file = open('rankfm.obj', 'wb')\n",
    "# pickle.dump(best_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred = test.copy()\n",
    "# test['itemID'] = test['gameID']\n",
    "# # Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "# test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "# test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "# test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "# test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "# scores = best_model.predict(test[['userIDX', 'itemIDX']])\n",
    "# testpred = pd.read_csv('./pairs_Played.csv')\n",
    "# testpred['prediction'] = (scores >= 0).astype(np.int32)\n",
    "# testpred.to_csv('./predictions_Played.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.280727226141243"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlabel = df_time_train_label.to_numpy()\n",
    "mean = np.mean(tlabel)\n",
    "np.mean((tlabel - mean)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FactorizationMachine():\n",
    "    def __init__(self, n_components=10, n_user=0, n_item=0, feats=None) -> None:\n",
    "        self.n_components = n_components\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        if feats is not None:\n",
    "            self.n_feat = feats.shape[1]\n",
    "            self.feats = feats\n",
    "            self.f_linear = np.zeros(self.n_feat)\n",
    "            self.f_latent = np.zeros((self.n_feat, self.n_components))\n",
    "\n",
    "        self.u_latent = np.zeros((self.n_user, self.n_components))\n",
    "        self.i_latent = np.zeros((self.n_user, self.n_components))\n",
    "        self.beta_u = np.zeros(self.n_user)\n",
    "        self.beta_i = np.zeros(self.n_item)\n",
    "        self.alpha = 0\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        u_latent = self.u_latent[X[:, 0]]\n",
    "        i_latent = self.i_latent[X[:, 1]]\n",
    "\n",
    "    def partial_fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        u_latent = self.u_latent[X[:, 0]]\n",
    "        i_latent = self.i_latent[X[:, 1]]\n",
    "        preds += self.alpha\n",
    "        preds += self.beta_u[X[:, 0]]\n",
    "        preds += self.beta_i[X[:, 1]]\n",
    "        preds += self.f_linear @ self.feats[X[:, 1]].T\n",
    "        preds += np.sum(u_latent * i_latent, axis=1)\n",
    "        preds += np.sum((u_latent @ self.f_latent.T) * self.feats[X[:, 1]], axis=1)\n",
    "        return preds\n",
    "\n",
    "model = FactorizationMachine(10, 10, 10, np.ones((10, 10)))\n",
    "model.predict(np.array([[0,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make and write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('./pairs_Hours.csv')\n",
    "# testpred = test.copy()\n",
    "# test['itemID'] = test['gameID']\n",
    "# # Map unseen entries to default user (this user is already grouped with other users due to their few # of reviews in training set)\n",
    "# test['userID'] = test['userID'].map(lambda x: x if x in userset else 'u03473346')\n",
    "# test['userIDX'] = user_oe.transform(test[['userID']])\n",
    "# test['itemIDX'] = item_oe.transform(test[['gameID']])\n",
    "# test.drop(columns=['gameID', 'prediction'], inplace=True)\n",
    "\n",
    "# preds = []\n",
    "# for i in range(len(test)):\n",
    "#     row = test.iloc[i]\n",
    "#     itemIDX = row['itemIDX']\n",
    "#     # Build positive pair\n",
    "#     data = np.concatenate((row[2:].to_numpy().astype(np.float64),\n",
    "#                             text_embed[itemIDX][1:].astype(np.float64)))\n",
    "#     preds.append(time_model(torch.tensor(data).to(device)).item())\n",
    "\n",
    "\n",
    "# testpred = pd.read_csv('./pairs_Hours.csv')\n",
    "# testpred['prediction'] = preds\n",
    "# testpred.to_csv('./predictions_Hours.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

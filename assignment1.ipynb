{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258: Assignment 1\n",
    "### Benjamin Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:19.717930300Z",
     "start_time": "2023-10-26T01:59:57.594733500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16ef164e170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user/item ID's, compensation, early_access, and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:00:53.408937700Z",
     "start_time": "2023-10-26T02:00:34.962524200Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    f: gzip.GzipFile = gzip.open(path)\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = eval(line)\n",
    "        yield entry\n",
    "\n",
    "# Encode userID and itemID as integers\n",
    "def process_data() -> pd.DataFrame:\n",
    "    data = []\n",
    "    for entry in read_json('train.json.gz'):\n",
    "        data.append(entry)\n",
    "\n",
    "    df: pd.DataFrame = pd.DataFrame(data)\n",
    "\n",
    "    del data\n",
    "    oe = preprocessing.OrdinalEncoder(dtype=np.int32) # User/item id encoder\n",
    "    df[['userID', 'gameID']] = oe.fit_transform(df[['userID', 'gameID']])\n",
    "    df.rename({'gameID' : 'itemID'}, axis=1, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].map(lambda x : x.month)\n",
    "    df['year'] = df['date'].map(lambda x : x.year)\n",
    "    df['day_of_month'] = df['date'].map(lambda x : x.day)\n",
    "    df['day_of_wk'] = df['date'].map(lambda x : x.dayofweek)\n",
    "    df['day_of_yr'] = df['date'].map(lambda x : x.dayofyear)\n",
    "    df['wk_of_yr'] = df['date'].map(lambda x : x.weekofyear)\n",
    "    df.fillna(value=0, axis=1, inplace=True)\n",
    "    df['compensation'] = df['compensation'].map(lambda x : x if x == 0 else 1)\n",
    "    df[['early_access', 'compensation']] = df[['early_access', 'compensation']].astype(np.int32)\n",
    "    mme = preprocessing.MinMaxScaler()\n",
    "    df[['month', 'year', 'day_of_month', 'day_of_wk', 'day_of_yr', 'wk_of_yr']] = mme.fit_transform(df[['month', 'year', 'day_of_month', 'day_of_wk', 'day_of_yr', 'wk_of_yr']])\n",
    "    df.drop(labels=['hours', 'user_id', 'date'], axis=1, inplace=True)\n",
    "    df.head()\n",
    "    return df\n",
    "\n",
    "df = process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess user text and convert to descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# NOTE: Using pretrained sentiment similarity transformer\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_text_embedding():\n",
    "    if not os.path.isfile('./text_embed.npy'): # Generate new descriptors for each review using pretrained transformer\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        transformer = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "        text_embed = np.zeros((len(df), 384))\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(df), 1)):\n",
    "                encoded_input = tokenizer(df.iloc[i:i+1]['text'].tolist(),\n",
    "                                        padding=True,\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt').to(device)\n",
    "                model_output = transformer(**encoded_input)\n",
    "                embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                embeddings: torch.Tensor = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "                text_embed[i:i+1] = embeddings.cpu().numpy()\n",
    "        np.save('text_embed.npy', text_embed)\n",
    "    else: # Text descriptors already computed\n",
    "        text_embed = np.load('./text_embed.npy')\n",
    "\n",
    "    return text_embed\n",
    "\n",
    "text_embed = get_text_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_feature, latent_dim) -> None:\n",
    "        \"\"\"\n",
    "        n_user: Number of unique users\n",
    "        n_item: Number of unique items\n",
    "        n_feature: Number of extra features to use\n",
    "        latent_dim: Dimension of latent representations of users/items/features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.n_feature = n_feature\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.user_latent = nn.Embedding(n_user, latent_dim)\n",
    "        self.item_latent = nn.Embedding(n_item, latent_dim)\n",
    "        self.feat_latent = nn.Embedding(n_feature, latent_dim)\n",
    "        self.user_weight = nn.Embedding(n_user, 1)\n",
    "        self.item_weight = nn.Embedding(n_item, 1)\n",
    "        # \"alpha\" or \"w_0\" term will be absorbed into feat_weight linear's bias\n",
    "        self.feat_weight = nn.Linear(n_feature, 1)\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input shape: batch_size x (user idx, item idx, features) - 2 dimensional\n",
    "        Returns: n x 1 tensor of predictions\n",
    "        \"\"\"\n",
    "        # f(u, i) = w_0 + \\sum_{j=1}^{d} w_j * x_j\n",
    "        out = self.feat_weight(x[:, 2:])\n",
    "        out += self.user_weight(x[:, 0].to(dtype=torch.int32))\n",
    "        out += self.item_weight(x[:, 1].to(dtype=torch.int32))\n",
    "        # Nested summation thingy\n",
    "        u_embed = self.user_latent(x[:, 0].to(dtype=torch.int32))\n",
    "        i_embed = self.item_latent(x[:, 1].to(dtype=torch.int32))\n",
    "        f_embed = self.feat_latent(torch.Tensor(range(0, self.n_feature)).to(device, dtype=torch.int32))\n",
    "        out += (u_embed * i_embed).sum(dim=1).unsqueeze(-1)\n",
    "        out += (u_embed @ f_embed.T).sum(dim=1).unsqueeze(-1)\n",
    "        out += (i_embed @ f_embed.T).sum(dim=1).unsqueeze(-1)\n",
    "        for i in range(0, self.n_feature):\n",
    "            for j in range(0, i):\n",
    "                out += (f_embed[i] @ f_embed[j].T) * (x[:, 2 + i] * x[:, 2 + j]).unsqueeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5396],\n",
       "        [1.1414],\n",
       "        [6.9715],\n",
       "        [4.4861],\n",
       "        [3.5364],\n",
       "        [2.8154],\n",
       "        [5.3209],\n",
       "        [2.4965],\n",
       "        [4.4025],\n",
       "        [1.8168]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FactorizationMachine(2, 2, 3, 5).to(device)\n",
    "ui = torch.randint(0, 2, (10, 2))\n",
    "d = torch.concatenate([ui, torch.randn((10, 3))], dim=1).to(device)\n",
    "model(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
